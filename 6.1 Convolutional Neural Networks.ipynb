{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6.1 Convolutional Neural Networks.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "D2bKwlhxB-DC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "outputId": "9f9bbc34-1bf6-4e52-89e9-03c8a2ec8170"
      },
      "cell_type": "code",
      "source": [
        "!pip install xlrd\n",
        "!wget 'https://github.com/ameasure/autocoding-class/raw/master/msha.xlsx'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting xlrd\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/e6/e95c4eec6221bfd8528bcc4ea252a850bffcc4be88ebc367e23a1a84b0bb/xlrd-1.1.0-py2.py3-none-any.whl (108kB)\n",
            "\r\u001b[K    9% |███                             | 10kB 11.7MB/s eta 0:00:01\r\u001b[K    18% |██████                          | 20kB 2.4MB/s eta 0:00:01\r\u001b[K    28% |█████████                       | 30kB 2.8MB/s eta 0:00:01\r\u001b[K    37% |████████████                    | 40kB 2.9MB/s eta 0:00:01\r\u001b[K    47% |███████████████                 | 51kB 3.2MB/s eta 0:00:01\r\u001b[K    56% |██████████████████              | 61kB 3.8MB/s eta 0:00:01\r\u001b[K    65% |█████████████████████           | 71kB 3.8MB/s eta 0:00:01\r\u001b[K    75% |████████████████████████        | 81kB 4.2MB/s eta 0:00:01\r\u001b[K    84% |███████████████████████████     | 92kB 4.2MB/s eta 0:00:01\r\u001b[K    94% |██████████████████████████████  | 102kB 4.4MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 112kB 4.3MB/s \n",
            "\u001b[?25hInstalling collected packages: xlrd\n",
            "Successfully installed xlrd-1.1.0\n",
            "--2018-10-11 16:51:12--  https://github.com/ameasure/autocoding-class/raw/master/msha.xlsx\n",
            "Resolving github.com (github.com)... 192.30.253.112, 192.30.253.113\n",
            "Connecting to github.com (github.com)|192.30.253.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/ameasure/autocoding-class/master/msha.xlsx [following]\n",
            "--2018-10-11 16:51:13--  https://raw.githubusercontent.com/ameasure/autocoding-class/master/msha.xlsx\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4183086 (4.0M) [application/octet-stream]\n",
            "Saving to: ‘msha.xlsx’\n",
            "\n",
            "msha.xlsx           100%[===================>]   3.99M  11.6MB/s    in 0.3s    \n",
            "\n",
            "2018-10-11 16:51:13 (11.6 MB/s) - ‘msha.xlsx’ saved [4183086/4183086]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sZW8pscPBg-o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Networks\n",
        "\n",
        "In our previous class we introduced the densely connected neural network with a single hidden layer. I pointed out that there has been some theoretical work showing such a model can approximate any relationship between the inputs and outputs to an arbitrary degree. None the less, when we tried it out on MSHA coding we saw no significant improvements. Why?\n",
        "\n",
        "The problem is that it is not enough for a model to be theoretically capable of representing any relationship, it must also be able to learn that relationship from the training data that is available. Here, there are no guarantees. The more flexible our model is, the more possibilities it has to rule out based on the training data. Conversely, the more correctly we can specify the model up front, the less burden we place on the training data. This is sometimes called an \"inductive bias.\" One of the big advantages of deep neural networks is that they give us a lot of flexibility in specifying these biases.  \n",
        "\n",
        "So what sorts of biases might exist in our data? Obviously it depends on the task, but there are some biases that seem to be very common in nature. One of these is locality. Locality simply means that things that are close to each other are much more likely to interact with each other. This occurs at all sorts of levels in all sorts of ways. If you're an antelope on the savannah, a lion right next to you is a serious threat, a lion 10,000 miles away is completely irrelevant. If you're a particle hurtling through space, the fact that there's a planet a million miles away is much less relevant to your flight path then the fact that there's another particle just in front of you that you're about to collide with. If you're a pixel on an image, the pixel right next to you is much more likely to be similar to you than the one on the other side of the image. If you're a word in a document, your meaning is much more likely to be affected by other words in the same sentence than by words in some distant paragraph. Locality is basically everywhere in the natural world yet our densely connected neural network has no such bias, it simply allows every input to interact with every other input and all of these are treated initially as equally likely possibilities. So an important question is, can we constrain our model to better reflect the bias toward locality? The answer is yes. One way to do this is with a convolutional layer.\n",
        "\n",
        "In its simplest form, a convolutional layer is simply one or more densely connected artificial neurons that are applied to subsets of the input. By only allowing these neurons to operate on small subsets of the input, we are essentially forcing them to learn to recognize smaller structures that may exist. Information about these structures can then be passed to later layers (potentially also convolutional) to form a larger understanding of the input. \n",
        "\n",
        "This idea has enjoyed enormous success in computer vision  and is now used extensively in essentially every vision processing task. But imagery is not the only input that exhibits locality and compositionality. Language also has this. For example, words that occur next to each other in a sentence are far more likely to be related to each other. The same is true for letters, and if we want to think about the larger structures, words are made of letters, phrases are made of words, sentences are made of phrases, and documents are made of sentences. Some of the same insights from vision also apply to language.\n",
        "\n",
        "Consider, for example, the sentence \"the man fell on his left side.\" If we were to represent each of the words in this sentence by a vector, we could represent the entire input by the concatenation of these vectors. We could then perform a convolutional operation across each 3 word sub-sequence in this sentence as follows:\n",
        "\n",
        "![Images](https://github.com/ameasure/colab_tutorials/blob/master/Images/1d_conv_nopool_loop.gif?raw=1)\n",
        "\n",
        "Note, in this particular convolutional layer we have two artificial neurons (also known as filters). Like any articial neuron each filter has weights, controlling how each input contributes to the final output, and a bias. As each filter is applied to each continuous 3-word sequence in our sentence, it generates an output. The final result is that for just this one sentence, each filter has generated 6 outputs corresponding to the 6 continuous 3-word sequences found in our sentence. Because there tends to be lots of redundant information in the output of each filter, it is common to aggregate this information. One approach is max pooling, i.e. simply  take the highest value produced by each filter. The result is a single vector of output containing 2 values, corresponding to the highest values produced by each of our filters. The entire computation is illustrated below:\n",
        "\n",
        "![Images](https://github.com/ameasure/colab_tutorials/blob/master/Images/1d_conv_withpool_loop.gif?raw=1)"
      ]
    },
    {
      "metadata": {
        "id": "-m22ZczGBg-6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We implement this model below, in Keras. \n",
        "\n",
        "### Preparing the Data\n",
        "\n",
        "The first step is to prepare the input data and here we diverge from our previous approach. In the past we used the bag-of-words approach, discarding all information about the order in which words appear. Now that we're working with convolutions, we need to preserve this information. We will accomplish this by using the Keras Tokenizer to map each word to a unique number, and then representing the sequence of words in each our narratives by the corresponding sequence of numbers. Although this ends up happening behind the scenes, this is equivalent to representing each word with a one-hot-encoding and stacking the one-hot-encodings sequentially."
      ]
    },
    {
      "metadata": {
        "id": "O4msdvfvBg-8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "49007460-d86c-454b-e081-29e370d967d9"
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "df = pd.read_excel('msha.xlsx')\n",
        "df['ACCIDENT_YEAR'] = df['ACCIDENT_DT'].apply(lambda x: x.year)\n",
        "df['ACCIDENT_YEAR'].value_counts()\n",
        "df_train = df[df['ACCIDENT_YEAR'].isin([2010, 2011])].copy()\n",
        "df_valid = df[df['ACCIDENT_YEAR'] == 2012].copy()\n",
        "print('training rows:', len(df_train))\n",
        "print('validation rows:', len(df_valid))\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df_train['NARRATIVE'])\n",
        "X_train_seq = tokenizer.texts_to_sequences(df_train['NARRATIVE'])\n",
        "X_valid_seq = tokenizer.texts_to_sequences(df_valid['NARRATIVE'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training rows: 18681\n",
            "validation rows: 9032\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "onRKmYKSBg_E",
        "colab_type": "code",
        "colab": {},
        "outputId": "c7f1691b-fb47-4c72-f410-64118ef32bb7"
      },
      "cell_type": "code",
      "source": [
        "print(X_train_seq[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[244, 29, 7152, 1570, 764, 213, 970, 4, 3198, 139, 5, 1924, 424, 223, 610, 1, 764, 29, 10, 1, 1570, 9, 3, 64, 2, 490, 110, 5, 213, 1, 764, 813, 4, 164, 317, 11, 6, 15, 54]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UzSjdinDBg_L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As you can see, the Keras tokenizer has converted our narrative into a list of numbers, each corresponding to a word. There is, however, one more modification we need to make. Because each narrative contains a different number of words, but all our neural network layers contain a fixed number of weights, we need to figure out what to do with the mismatch. The simplest approach is simply to pad each narrative to the same length with special \"blank\" words (representer by the number 0). We accomplish this using the pad_sequences function from Keras, padding each narrative to 200 words (or truncating it to 200 words, if it is longer). "
      ]
    },
    {
      "metadata": {
        "id": "lfsl1d9tBg_N",
        "colab_type": "code",
        "colab": {},
        "outputId": "141fdd69-035e-4d71-e1a3-a872c84120c9"
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing import sequence\n",
        "\n",
        "X_train_seq = sequence.pad_sequences(X_train_seq, maxlen=200)\n",
        "X_valid_seq = sequence.pad_sequences(X_valid_seq, maxlen=200)\n",
        "\n",
        "print(X_train_seq[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0  244   29 7152 1570  764  213  970\n",
            "    4 3198  139    5 1924  424  223  610    1  764   29   10    1 1570\n",
            "    9    3   64    2  490  110    5  213    1  764  813    4  164  317\n",
            "   11    6   15   54]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IkTTaWQdBg_U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Our training inputs are now ready, we just need to prepare the training outputs. Keras requires these to be in a one-hot encoding. We do that below using sklearn's LabelBinarizer."
      ]
    },
    {
      "metadata": {
        "id": "qJDa7OkwBg_W",
        "colab_type": "code",
        "colab": {},
        "outputId": "4d872463-eec2-4e44-a721-857132a6c0ec"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "# keras only accepts a one-hot encoding of the training labels\n",
        "# we do that here\n",
        "label_encoder = LabelBinarizer().fit(df_train['INJ_BODY_PART'])\n",
        "y_train = label_encoder.transform(df_train['INJ_BODY_PART'])\n",
        "y_valid = label_encoder.transform(df_valid['INJ_BODY_PART'])\n",
        "n_codes = len(label_encoder.classes_)\n",
        "print(y_train[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
            " 0 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XQS3OpllBg_d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We're now ready to specify the convolutional model. Here we use a single convolutional layer with 100 filters, each operating over 3-word subsets of the input."
      ]
    },
    {
      "metadata": {
        "id": "I04l3yN0Bg_f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Dense, Input, Dropout\n",
        "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Concatenate\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "input_text = Input(shape=(200,), dtype='int32')\n",
        "embedding = Embedding(len(tokenizer.word_index), \n",
        "                          300, \n",
        "                          input_length=200)(input_text)\n",
        "dropout = Dropout(0.1)(embedding)\n",
        "convolution = Conv1D(filters=100, \n",
        "                     kernel_size=3,\n",
        "                     padding='valid',\n",
        "                     strides=1,\n",
        "                     activation='relu')(dropout)\n",
        "pool = GlobalMaxPooling1D()(convolution)\n",
        "dense = Dense(100, activation='relu')(pool)\n",
        "dropout = Dropout(0.5)(dense)\n",
        "output = Dense(len(label_encoder.classes_), activation='softmax')(dense)\n",
        "\n",
        "conv_model = Model(inputs=input_text, outputs=output)\n",
        "\n",
        "conv_model.compile(optimizer='adam', \n",
        "                  loss='categorical_crossentropy', \n",
        "                  metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "II_9oecSBg_j",
        "colab_type": "code",
        "colab": {},
        "outputId": "90773c75-02bc-4b91-c4cd-8eb5045ba0b6"
      },
      "cell_type": "code",
      "source": [
        "conv_model.fit(x=X_train_seq, y=y_train,\n",
        "               validation_data=(X_valid_seq, y_valid),\n",
        "               batch_size=32, epochs=5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 18681 samples, validate on 9032 samples\n",
            "Epoch 1/5\n",
            "18681/18681 [==============================] - 7s 366us/step - loss: 1.5671 - acc: 0.6097 - val_loss: 0.8923 - val_acc: 0.7603\n",
            "Epoch 2/5\n",
            "18681/18681 [==============================] - 4s 190us/step - loss: 0.7887 - acc: 0.7830 - val_loss: 0.8033 - val_acc: 0.7835\n",
            "Epoch 3/5\n",
            "18681/18681 [==============================] - 4s 190us/step - loss: 0.5697 - acc: 0.8364 - val_loss: 0.8139 - val_acc: 0.7840\n",
            "Epoch 4/5\n",
            "18681/18681 [==============================] - 4s 190us/step - loss: 0.3840 - acc: 0.8926 - val_loss: 0.8940 - val_acc: 0.7773\n",
            "Epoch 5/5\n",
            "18681/18681 [==============================] - 4s 190us/step - loss: 0.2355 - acc: 0.9372 - val_loss: 1.0102 - val_acc: 0.7715\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff73146f470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "2qv95Dy1Bg_s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "There's nothing magical about convolving over 3-word subsequences, an alternate is to have multiple convolutional layers each operating over different length subsequences. Here, we create convolutional layers for 2, 3, 4, and 5 word subsequences. The resulting outputs are then concatenated before being fed to subsequent layers."
      ]
    },
    {
      "metadata": {
        "id": "L7z6xF06Bg_u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "input_text = Input(shape=(200,), dtype='int32')\n",
        "embedding = Embedding(len(tokenizer.word_index), \n",
        "                          300, \n",
        "                          input_length=200)(input_text)\n",
        "dropout = Dropout(0.1)(embedding)\n",
        "pooled_convolutions = []\n",
        "for kernel_size in [2, 3, 4, 5]:\n",
        "    convolution = Conv1D(filters=20, \n",
        "                         kernel_size=kernel_size,\n",
        "                         padding='valid',\n",
        "                         strides=1,\n",
        "                         activation='relu')(dropout)\n",
        "    pool = GlobalMaxPooling1D()(convolution)\n",
        "    pooled_convolutions.append(pool)\n",
        "concatenated = Concatenate()(pooled_convolutions)\n",
        "dropout = Dropout(0.5)(concatenated)\n",
        "dense = Dense(100, activation='relu')(dropout)\n",
        "dropout = Dropout(0.5)(dense)\n",
        "output = Dense(len(label_encoder.classes_), activation='softmax')(dense)\n",
        "\n",
        "conv_model = Model(inputs=input_text, outputs=output)\n",
        "\n",
        "conv_model.compile(optimizer='adam', \n",
        "                  loss='categorical_crossentropy', \n",
        "                  metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yzuDoDJnBg_0",
        "colab_type": "code",
        "colab": {},
        "outputId": "e9f4846b-4e11-4925-e9b1-94413914dc45"
      },
      "cell_type": "code",
      "source": [
        "conv_model.fit(x=X_train_seq, y=y_train,\n",
        "               validation_data=(X_valid_seq, y_valid),\n",
        "               batch_size=32, epochs=5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 18681 samples, validate on 9032 samples\n",
            "Epoch 1/5\n",
            "18681/18681 [==============================] - 6s 320us/step - loss: 1.9869 - acc: 0.5084 - val_loss: 1.0721 - val_acc: 0.7377\n",
            "Epoch 2/5\n",
            "18681/18681 [==============================] - 5s 248us/step - loss: 1.1448 - acc: 0.7113 - val_loss: 0.8505 - val_acc: 0.7790\n",
            "Epoch 3/5\n",
            "18681/18681 [==============================] - 5s 249us/step - loss: 0.9172 - acc: 0.7601 - val_loss: 0.7912 - val_acc: 0.7868\n",
            "Epoch 4/5\n",
            "18681/18681 [==============================] - 5s 249us/step - loss: 0.8030 - acc: 0.7892 - val_loss: 0.8054 - val_acc: 0.7871\n",
            "Epoch 5/5\n",
            "18681/18681 [==============================] - 5s 248us/step - loss: 0.7231 - acc: 0.8056 - val_loss: 0.8172 - val_acc: 0.7857\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff6fce66dd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    }
  ]
}