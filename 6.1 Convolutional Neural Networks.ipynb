{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "Perhaps the most important advantage of the deep neural network is that it allows us add layers of abstraction to our model between the input and the output. This in turn gives us great flexibility in specifying how our inputs might relate to our outputs. These inductive biases, as they're sometimes called, have proven to be very helpful in modeling complex phenomena such as imagery, speech, and language.\n",
    "\n",
    "So what sorts of biases might exist in our data? It depends on the task of course, but two biases that are very common in nature are locality and compositionality. In other words, things that are close to each other are more likely to be related to each other, and big things can often be more succintly described as a collection of smaller things. Consider, for example, an image. In its raw form, an image is simply a matrix of pixels. But these pixels are not equally likely to be related to each other. Pixels that are next to each other are more likely to belong to the same object, or shape. And objects and shapes that are next to each other, in turn, are more likely to describe larger objects and shapes that are also nearby. A densely connected neural network layer has no such bias, it allows for all pixels to interact with all pixels to form some output. Can do we better by constraining our model to better reflect locality and compositionality? The answer is often yes, and one popular way to do this is with a convolutional layer.\n",
    "\n",
    "In its simplest form, a convolutional layer is simply one or more densely connected layers that are applied to subsets of the input. By only allowing these densely connected layers to operate on small subsets of the input, we are essentially forcing them to learn to recognize smaller structures that may exist. Information about these structures can then be passed to later layers (potentially also convolutional) to form a larger understanding of the input. This idea has enjoyed enormous success in computer vision and it is now used extensively in essentially every vision processing task. But imagery is not the only input that exhibits locality and compositionality. Language also has this. For example, words that occur next to each other in a sentence are far more likely to be related to each other. The same is true for letters, and if we want to think about the larger structures, words are made of letters, phrases are made of words, sentences are made of phrases, and documents are made of sentences. Some of the same insights from vision also apply to language.\n",
    "\n",
    "Consider, for example, the sentence \"the man fell on his left side.\" If we were to represent each of the words in this sentence by a vector, we could represent the entire input by the concatenation of these vectors. We could then perform a convolutional operation across each 3 word sub-sequence in this sentence as follows:\n",
    "\n",
    "![Images](Images/numeric_1d_conv_animation.gif)\n",
    "\n",
    "Note, in this particular convolutional layer we have two dense layers (also known as filters). Like any dense layer each filter has weights, controlling how each input contributes to the final output, and a bias. As each dense layer is applied to each continuous 3-word sequence in our sentence, it generates an output. The final result is that for just this one sentence, each filter has generated 6 outputs corresponding to the 6 continuous 3-word sequences found in our sentence. Because there tends to be lots of redundant information in the output of each filter, it is common to aggregate this information. One approach is max pooling, i.e. simply  take the highest value produced by each filter. The result is a single vector of output containing 2 values, corresponding to the highest values produced by each of our filters. The entire computation is illustrated below:\n",
    "\n",
    "![Images](Images/one_dim_conv_anim_continuous.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement this model below, in Keras. \n",
    "\n",
    "### Preparing the Data\n",
    "\n",
    "The first step is to prepare the input data and here we diverge from our previous approach. In the past we used the bag-of-words approach, discarding all information about the order in which words appear. Now that we're working with convolutions, we need to preserve this information. We will accomplish this by using the Keras Tokenizer to map each word to a unique number, and then representing the sequence of words in each our narratives by the corresponding sequence of numbers. Although this ends up happening behind the scenes, this is equivalent to representing each word with a one-hot-encoding and stacking the one-hot-encodings sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# read in our training data\n",
    "df_train = pd.read_hdf('Data/msha_2010-2011.h5')\n",
    "# read in our validation data\n",
    "df_valid = pd.read_hdf('Data/msha_2012.h5')\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df_train['NARRATIVE'])\n",
    "X_train_seq = tokenizer.texts_to_sequences(df_train['NARRATIVE'])\n",
    "X_valid_seq = tokenizer.texts_to_sequences(df_valid['NARRATIVE'])\n",
    "\n",
    "# keras only accepts a one-hot encoding of the training labels\n",
    "# we do that here\n",
    "label_encoder = LabelBinarizer().fit(df_train['INJ_BODY_PART'])\n",
    "y_train = label_encoder.transform(df_train['INJ_BODY_PART'])\n",
    "y_valid = label_encoder.transform(df_valid['INJ_BODY_PART'])\n",
    "n_codes = len(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[244, 29, 7152, 1570, 764, 213, 970, 4, 3198, 139, 5, 1924, 424, 223, 610, 1, 764, 29, 10, 1, 1570, 9, 3, 64, 2, 490, 110, 5, 213, 1, 764, 813, 4, 164, 317, 11, 6, 15, 54]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_seq[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the Keras tokenizer has converted our narrative into a list of numbers, each corresponding to a word. There is, however, one more modification we need to make. Because each narrative contains a different number of words, but all our neural network layers contain a fixed number of weights, we need to figure out what to do with the mismatch. The simplest approach is simply to pad each narrative to the same length with special \"blank\" words (representer by the number 0). We accomplish this using the pad_sequences function from Keras, padding each narrative to 200 words (or truncating it to 200 words, if it is longer). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "X_train_seq = sequence.pad_sequences(X_train_seq, maxlen=200)\n",
    "X_valid_seq = sequence.pad_sequences(X_valid_seq, maxlen=200)\n",
    "\n",
    "print(X_train_seq[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to specify the convolutional model. Here we use a single convolutional layer with 100 filters, each operating over 3-word subsets of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Concatenate\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "input_text = Input(shape=(200,), dtype='int32')\n",
    "embedding = Embedding(len(tokenizer.word_index), \n",
    "                          300, \n",
    "                          input_length=200)(input_text)\n",
    "dropout = Dropout(0.1)(embedding)\n",
    "convolution = Conv1D(filters=100, \n",
    "                     kernel_size=3,\n",
    "                     padding='valid',\n",
    "                     strides=1,\n",
    "                     activation='relu')(dropout)\n",
    "pool = GlobalMaxPooling1D()(convolution)\n",
    "dense = Dense(100, activation='relu')(pool)\n",
    "dropout = Dropout(0.5)(dense)\n",
    "output = Dense(len(label_encoder.classes_), activation='softmax')(dense)\n",
    "\n",
    "conv_model = Model(inputs=input_text, outputs=output)\n",
    "\n",
    "conv_model.compile(optimizer='adam', \n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18681 samples, validate on 9032 samples\n",
      "Epoch 1/5\n",
      "18681/18681 [==============================] - 7s 366us/step - loss: 1.5268 - acc: 0.6211 - val_loss: 0.8721 - val_acc: 0.7664\n",
      "Epoch 2/5\n",
      "18681/18681 [==============================] - 4s 190us/step - loss: 0.7802 - acc: 0.7903 - val_loss: 0.7860 - val_acc: 0.7881\n",
      "Epoch 3/5\n",
      "18681/18681 [==============================] - 4s 190us/step - loss: 0.5623 - acc: 0.8421 - val_loss: 0.8078 - val_acc: 0.7886\n",
      "Epoch 4/5\n",
      "18681/18681 [==============================] - 4s 190us/step - loss: 0.3745 - acc: 0.8979 - val_loss: 0.9033 - val_acc: 0.7752\n",
      "Epoch 5/5\n",
      "18681/18681 [==============================] - 4s 190us/step - loss: 0.2222 - acc: 0.9417 - val_loss: 1.0007 - val_acc: 0.7754\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8bc121f358>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model.fit(x=X_train_seq, y=y_train,\n",
    "               validation_data=(X_valid_seq, y_valid),\n",
    "               batch_size=32, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's nothing magical about convolving over 3-word subsequences, an alternate is to have multiple convolutional layers each operating over different length subsequences. Here, we create convolutional layers for 2, 3, 4, and 5 word subsequences. The resulting outputs are then concatenated before being fed to subsequent layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = Input(shape=(200,), dtype='int32')\n",
    "embedding = Embedding(len(tokenizer.word_index), \n",
    "                          300, \n",
    "                          input_length=200)(input_text)\n",
    "dropout = Dropout(0.1)(embedding)\n",
    "pooled_convolutions = []\n",
    "for kernel_size in [2, 3, 4, 5]:\n",
    "    convolution = Conv1D(filters=20, \n",
    "                         kernel_size=kernel_size,\n",
    "                         padding='valid',\n",
    "                         strides=1,\n",
    "                         activation='relu')(dropout)\n",
    "    pool = GlobalMaxPooling1D()(convolution)\n",
    "    pooled_convolutions.append(pool)\n",
    "concatenated = Concatenate()(pooled_convolutions)\n",
    "dropout = Dropout(0.5)(concatenated)\n",
    "dense = Dense(100, activation='relu')(dropout)\n",
    "dropout = Dropout(0.5)(dense)\n",
    "output = Dense(len(label_encoder.classes_), activation='softmax')(dense)\n",
    "\n",
    "conv_model = Model(inputs=input_text, outputs=output)\n",
    "\n",
    "conv_model.compile(optimizer='adam', \n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18681 samples, validate on 9032 samples\n",
      "Epoch 1/5\n",
      "18681/18681 [==============================] - 6s 319us/step - loss: 1.9515 - acc: 0.5211 - val_loss: 1.0385 - val_acc: 0.7401\n",
      "Epoch 2/5\n",
      "18681/18681 [==============================] - 5s 249us/step - loss: 1.1282 - acc: 0.7137 - val_loss: 0.8412 - val_acc: 0.7841\n",
      "Epoch 3/5\n",
      "18681/18681 [==============================] - 5s 248us/step - loss: 0.9257 - acc: 0.7610 - val_loss: 0.8078 - val_acc: 0.7852\n",
      "Epoch 4/5\n",
      "18681/18681 [==============================] - 5s 248us/step - loss: 0.8099 - acc: 0.7831 - val_loss: 0.8056 - val_acc: 0.7869\n",
      "Epoch 5/5\n",
      "18681/18681 [==============================] - 5s 249us/step - loss: 0.7332 - acc: 0.8041 - val_loss: 0.8148 - val_acc: 0.7879\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8bbe8a76d8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model.fit(x=X_train_seq, y=y_train,\n",
    "               validation_data=(X_valid_seq, y_valid),\n",
    "               batch_size=32, epochs=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
