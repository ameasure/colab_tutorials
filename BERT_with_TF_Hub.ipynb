{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT with TF Hub.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ameasure/colab_tutorials/blob/master/BERT_with_TF_Hub.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l913GLvAYfis",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "65f58d5c-52d9-41ce-e73d-1fb6b621edf1"
      },
      "source": [
        "!pip install bert-tensorflow\n",
        "!pip install --upgrade pandas\n",
        "!wget --no-clobber 'https://github.com/ameasure/autocoding-class/raw/master/msha.xlsx'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bert-tensorflow in /usr/local/lib/python3.6/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n",
            "Requirement already up-to-date: pandas in /usr/local/lib/python3.6/dist-packages (0.24.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.16.4)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.5.3)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.5.0->pandas) (1.12.0)\n",
            "File ‘msha.xlsx’ already there; not retrieving.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sjjSQydkROH",
        "colab_type": "code",
        "outputId": "2f2a41f3-6fb6-406e-c6e8-8fd52aabe426",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel('msha.xlsx')\n",
        "df['ACCIDENT_YEAR'] = df['ACCIDENT_DT'].apply(lambda x: x.year)\n",
        "df['ACCIDENT_YEAR'].value_counts()\n",
        "df_train = df[df['ACCIDENT_YEAR'].isin([2010, 2011])].copy().sample(1000)\n",
        "df_valid = df[df['ACCIDENT_YEAR'] == 2012].copy().sample(1000)\n",
        "print('training rows:', len(df_train))\n",
        "print('validation rows:', len(df_valid))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training rows: 1000\n",
            "validation rows: 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQ9v31Z1i2Uv",
        "colab_type": "text"
      },
      "source": [
        "Convert the labels to 1-hot arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ee_Z2Yasdcwp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "labeler = LabelEncoder().fit(df['INJ_BODY_PART'])\n",
        "df_train['LABEL'] = labeler.transform(df_train['INJ_BODY_PART'])\n",
        "df_valid['LABEL'] = labeler.transform(df_valid['INJ_BODY_PART'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WFF744sYuWv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "de775874-2fea-430e-9401-95c7903ab443"
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import backend as K\n",
        "import tensorflow_hub as hub\n",
        "import bert\n",
        "from bert import run_classifier\n",
        "from bert import optimization\n",
        "from bert import tokenization"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0613 22:13:07.845517 139730755979136 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2ePJM6mY27P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# preprocess data, convert each row into a bert \"InputExample\" object\n",
        "def bert_preprocess(row, axis=None):\n",
        "  return bert.run_classifier.InputExample(guid=None,\n",
        "                                          text_a=row['NARRATIVE'],\n",
        "                                          text_b=None,\n",
        "                                          label=row['INJ_BODY_PART'])\n",
        "\n",
        "processed_train = df_train.apply(bert_preprocess, axis=1)\n",
        "processed_valid = df_valid.apply(bert_preprocess, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzXTzERbar6Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the tokenizer for our BERT MODEL\n",
        "# path to BERT MODEL\n",
        "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
        "\n",
        "with tf.Graph().as_default():\n",
        "  # load the model from tensorflow hub\n",
        "  bert_module = hub.Module(BERT_MODEL_HUB)\n",
        "  # get the vocab file and do_lower_case function from the module\n",
        "  with tf.Session() as sess:\n",
        "    tokenization_info = bert_module(signature='tokenization_info', as_dict=True)\n",
        "    vocab_file = sess.run(tokenization_info['vocab_file'])\n",
        "    do_lower_case = sess.run(tokenization_info['do_lower_case'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gmp1de9sclCk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "ca5dad42-d639-4782-e2cb-78aecd8dfdab"
      },
      "source": [
        "tokenizer = bert.tokenization.FullTokenizer(vocab_file=vocab_file,\n",
        "                                            do_lower_case=do_lower_case)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0613 22:13:11.776346 139730755979136 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jgr2vVkPfJHi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "2b9e0a59-48c2-4e50-c4df-13c153ad249e"
      },
      "source": [
        "tokenizer.tokenize('EE was loading a Gabion Grizzly when he was struck by falling debris')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ee',\n",
              " 'was',\n",
              " 'loading',\n",
              " 'a',\n",
              " 'ga',\n",
              " '##bio',\n",
              " '##n',\n",
              " 'gr',\n",
              " '##izzly',\n",
              " 'when',\n",
              " 'he',\n",
              " 'was',\n",
              " 'struck',\n",
              " 'by',\n",
              " 'falling',\n",
              " 'debris']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JfXGdAggCWc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "a37260a7-7607-4910-9583-269a57f4a0c2"
      },
      "source": [
        "# Now we'll convert our inputs to the numeric representation that BERT expects,\n",
        "# a list of feature objects. Each feature object has 4 attributes:\n",
        "# input_ids = a list of numbers representing words in our narrative\n",
        "# input_mask = a list of 1/0s indicating words which are masked (for training only)\n",
        "# segment_ids = a list of 1/0s indicating which sequence each token belongs to (for multi-segment tasks)\n",
        "# label_id = id indicating the code for this example\n",
        "MAX_SEQ_LENGTH = 128\n",
        "LABELS = df['INJ_BODY_PART'].unique()\n",
        "train_features = bert.run_classifier.convert_examples_to_features(processed_train, \n",
        "                                                                  LABELS, \n",
        "                                                                  MAX_SEQ_LENGTH,\n",
        "                                                                  tokenizer)\n",
        "valid_features = bert.run_classifier.convert_examples_to_features(processed_valid, \n",
        "                                                                  LABELS, \n",
        "                                                                  MAX_SEQ_LENGTH,\n",
        "                                                                  tokenizer)\n",
        "\n",
        "train_input_ids, train_input_mask, train_segment_ids, train_label_id = [], [], [], []\n",
        "for f in train_features:\n",
        "  train_input_ids.append(f.input_ids)\n",
        "  train_input_mask.append(f.input_mask)\n",
        "  train_segment_ids.append(f.segment_ids)\n",
        "  train_label_id.append(f.label_id)\n",
        "  \n",
        "valid_input_ids, valid_input_mask, valid_segment_ids, valid_label_id = [], [], [], []\n",
        "for f in valid_features:\n",
        "  valid_input_ids.append(f.input_ids)\n",
        "  valid_input_mask.append(f.input_mask)\n",
        "  valid_segment_ids.append(f.segment_ids)\n",
        "  valid_label_id.append(f.label_id)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0613 22:13:11.922515 139730755979136 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Py02Z06g-hr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the Keras layer which will hold our BERT model\n",
        "class BertLayer(keras.layers.Layer):\n",
        "    def __init__(self, n_fine_tune_layers=10, **kwargs):\n",
        "        self.n_fine_tune_layers = n_fine_tune_layers\n",
        "        self.trainable = True\n",
        "        self.output_size = 768\n",
        "        super(BertLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.bert = hub.Module(\n",
        "            BERT_MODEL_HUB,\n",
        "            trainable=self.trainable,\n",
        "            name=\"{}_module\".format(self.name)\n",
        "        )\n",
        "        trainable_vars = self.bert.variables\n",
        "        \n",
        "        # Remove unused layers\n",
        "        trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
        "        \n",
        "        # Select how many layers to fine tune\n",
        "        trainable_vars = trainable_vars[-self.n_fine_tune_layers :]\n",
        "        \n",
        "        # Add to trainable weights\n",
        "        for var in trainable_vars:\n",
        "            self._trainable_weights.append(var)\n",
        "        \n",
        "        # Add non-trainable weights\n",
        "        for var in self.bert.variables:\n",
        "            if var not in self._trainable_weights:\n",
        "                self._non_trainable_weights.append(var)\n",
        "        \n",
        "        super(BertLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
        "        input_ids, input_mask, segment_ids = inputs\n",
        "        bert_inputs = dict(\n",
        "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
        "        )\n",
        "        result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
        "            \"pooled_output\"\n",
        "        ]\n",
        "        return result\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], self.output_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzvSfxJ2hMTL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "outputId": "6a573c74-e813-47d2-8daf-228c19e04aaf"
      },
      "source": [
        "def create_model():\n",
        "  # Build model\n",
        "  in_id = keras.layers.Input(shape=(MAX_SEQ_LENGTH,), name=\"input_ids\")\n",
        "  in_mask = keras.layers.Input(shape=(MAX_SEQ_LENGTH,), name=\"input_masks\")\n",
        "  in_segment = keras.layers.Input(shape=(MAX_SEQ_LENGTH,), name=\"segment_ids\")\n",
        "  bert_inputs = [in_id, in_mask, in_segment]\n",
        "\n",
        "  # Instantiate the custom Bert Layer defined above\n",
        "  bert_output = BertLayer(n_fine_tune_layers=1)(bert_inputs)\n",
        "\n",
        "  # Build the rest of the classifier \n",
        "  dense = keras.layers.Dense(256, activation='relu')(bert_output)\n",
        "  pred = keras.layers.Dense(len(LABELS), activation='softmax')(dense)\n",
        "\n",
        "  model = keras.models.Model(inputs=bert_inputs, outputs=pred) \n",
        "  model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "with tf.Graph().as_default():\n",
        "  sess = tf.Session()\n",
        "  K.set_session(sess)\n",
        "  model = create_model()\n",
        "  model.fit(\n",
        "      [train_input_ids, train_input_mask, train_segment_ids], \n",
        "      train_label_id,\n",
        "      validation_data=([valid_input_ids, valid_input_mask, valid_segment_ids], valid_label_id),\n",
        "      epochs=10,\n",
        "      batch_size=32\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0613 22:13:13.812247 139730755979136 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0613 22:13:13.818176 139730755979136 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0613 22:13:16.702840 139730755979136 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0613 22:13:16.750687 139730755979136 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0613 22:13:16.890019 139730755979136 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0613 22:13:16.923815 139730755979136 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/10\n",
            "1000/1000 [==============================] - 20s 20ms/step - loss: 3.3506 - acc: 0.1740 - val_loss: 3.1561 - val_acc: 0.2410\n",
            "Epoch 2/10\n",
            "1000/1000 [==============================] - 17s 17ms/step - loss: 2.8367 - acc: 0.2700 - val_loss: 2.9919 - val_acc: 0.2210\n",
            "Epoch 3/10\n",
            "1000/1000 [==============================] - 17s 17ms/step - loss: 2.5806 - acc: 0.2970 - val_loss: 2.8616 - val_acc: 0.2830\n",
            "Epoch 4/10\n",
            "1000/1000 [==============================] - 18s 18ms/step - loss: 2.3163 - acc: 0.3610 - val_loss: 2.7951 - val_acc: 0.3040\n",
            "Epoch 5/10\n",
            " 992/1000 [============================>.] - ETA: 0s - loss: 2.1039 - acc: 0.4032"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IS7_n7vAwnUS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}