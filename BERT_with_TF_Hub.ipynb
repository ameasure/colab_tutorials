{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT with TF Hub.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ameasure/colab_tutorials/blob/master/BERT_with_TF_Hub.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l913GLvAYfis",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "bcc8e745-c1cf-4ec0-d106-68714e781ef9"
      },
      "source": [
        "!pip install bert-tensorflow\n",
        "!pip install --upgrade pandas\n",
        "!wget --no-clobber 'https://github.com/ameasure/autocoding-class/raw/master/msha.xlsx'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bert-tensorflow in /usr/local/lib/python3.6/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n",
            "Requirement already up-to-date: pandas in /usr/local/lib/python3.6/dist-packages (0.24.2)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.16.4)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.5.3)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.5.0->pandas) (1.12.0)\n",
            "File ‘msha.xlsx’ already there; not retrieving.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sjjSQydkROH",
        "colab_type": "code",
        "outputId": "c9348584-9dd5-4f00-cbf9-d561980e8347",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel('msha.xlsx')\n",
        "df['ACCIDENT_YEAR'] = df['ACCIDENT_DT'].apply(lambda x: x.year)\n",
        "df['ACCIDENT_YEAR'].value_counts()\n",
        "df_train = df[df['ACCIDENT_YEAR'].isin([2010, 2011])].copy()\n",
        "df_valid = df[df['ACCIDENT_YEAR'] == 2012].copy()\n",
        "print('training rows:', len(df_train))\n",
        "print('validation rows:', len(df_valid))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training rows: 18681\n",
            "validation rows: 9032\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQ9v31Z1i2Uv",
        "colab_type": "text"
      },
      "source": [
        "Convert the labels to 1-hot arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ee_Z2Yasdcwp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "labeler = LabelEncoder().fit(df['INJ_BODY_PART'])\n",
        "df_train['LABEL'] = labeler.transform(df_train['INJ_BODY_PART'])\n",
        "df_valid['LABEL'] = labeler.transform(df_valid['INJ_BODY_PART'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WFF744sYuWv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "f8b83d97-9f19-455a-ad8d-a979ee7e3294"
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import backend as K\n",
        "import tensorflow_hub as hub\n",
        "import bert\n",
        "from bert import run_classifier\n",
        "from bert import optimization\n",
        "from bert import tokenization"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0613 22:48:45.152416 140714811475840 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2ePJM6mY27P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# preprocess data, convert each row into a bert \"InputExample\" object\n",
        "def bert_preprocess(row, axis=None):\n",
        "  return bert.run_classifier.InputExample(guid=None,\n",
        "                                          text_a=row['NARRATIVE'],\n",
        "                                          text_b=None,\n",
        "                                          label=row['INJ_BODY_PART'])\n",
        "\n",
        "processed_train = df_train.apply(bert_preprocess, axis=1)\n",
        "processed_valid = df_valid.apply(bert_preprocess, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzXTzERbar6Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the tokenizer for our BERT MODEL\n",
        "# path to BERT MODEL\n",
        "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
        "\n",
        "with tf.Graph().as_default():\n",
        "  # load the model from tensorflow hub\n",
        "  bert_module = hub.Module(BERT_MODEL_HUB)\n",
        "  # get the vocab file and do_lower_case function from the module\n",
        "  with tf.Session() as sess:\n",
        "    tokenization_info = bert_module(signature='tokenization_info', as_dict=True)\n",
        "    vocab_file = sess.run(tokenization_info['vocab_file'])\n",
        "    do_lower_case = sess.run(tokenization_info['do_lower_case'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gmp1de9sclCk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "514393e1-f561-4505-dfd4-bd6dcc1b37db"
      },
      "source": [
        "tokenizer = bert.tokenization.FullTokenizer(vocab_file=vocab_file,\n",
        "                                            do_lower_case=do_lower_case)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0613 22:48:49.787506 140714811475840 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jgr2vVkPfJHi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "421e6a23-a5a5-43de-f3c1-79bea266f2de"
      },
      "source": [
        "tokenizer.tokenize('EE was loading a Gabion Grizzly when he was struck by falling debris')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ee',\n",
              " 'was',\n",
              " 'loading',\n",
              " 'a',\n",
              " 'ga',\n",
              " '##bio',\n",
              " '##n',\n",
              " 'gr',\n",
              " '##izzly',\n",
              " 'when',\n",
              " 'he',\n",
              " 'was',\n",
              " 'struck',\n",
              " 'by',\n",
              " 'falling',\n",
              " 'debris']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JfXGdAggCWc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "bebcfd29-3645-4af4-e694-3b0ffca4ddd2"
      },
      "source": [
        "# Now we'll convert our inputs to the numeric representation that BERT expects,\n",
        "# a list of feature objects. Each feature object has 4 attributes:\n",
        "# input_ids = a list of numbers representing words in our narrative\n",
        "# input_mask = a list of 1/0s indicating words which are masked (for training only)\n",
        "# segment_ids = a list of 1/0s indicating which sequence each token belongs to (for multi-segment tasks)\n",
        "# label_id = id indicating the code for this example\n",
        "MAX_SEQ_LENGTH = 128\n",
        "LABELS = df['INJ_BODY_PART'].unique()\n",
        "train_features = bert.run_classifier.convert_examples_to_features(processed_train, \n",
        "                                                                  LABELS, \n",
        "                                                                  MAX_SEQ_LENGTH,\n",
        "                                                                  tokenizer)\n",
        "valid_features = bert.run_classifier.convert_examples_to_features(processed_valid, \n",
        "                                                                  LABELS, \n",
        "                                                                  MAX_SEQ_LENGTH,\n",
        "                                                                  tokenizer)\n",
        "\n",
        "train_input_ids, train_input_mask, train_segment_ids, train_label_id = [], [], [], []\n",
        "for f in train_features:\n",
        "  train_input_ids.append(f.input_ids)\n",
        "  train_input_mask.append(f.input_mask)\n",
        "  train_segment_ids.append(f.segment_ids)\n",
        "  train_label_id.append(f.label_id)\n",
        "  \n",
        "valid_input_ids, valid_input_mask, valid_segment_ids, valid_label_id = [], [], [], []\n",
        "for f in valid_features:\n",
        "  valid_input_ids.append(f.input_ids)\n",
        "  valid_input_mask.append(f.input_mask)\n",
        "  valid_segment_ids.append(f.segment_ids)\n",
        "  valid_label_id.append(f.label_id)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0613 22:48:49.947267 140714811475840 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Py02Z06g-hr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the Keras layer which will hold our BERT model\n",
        "class BertLayer(keras.layers.Layer):\n",
        "    def __init__(self, n_fine_tune_layers=10, **kwargs):\n",
        "        self.n_fine_tune_layers = n_fine_tune_layers\n",
        "        self.trainable = True\n",
        "        self.output_size = 768\n",
        "        super(BertLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.bert = hub.Module(\n",
        "            BERT_MODEL_HUB,\n",
        "            trainable=self.trainable,\n",
        "            name=\"{}_module\".format(self.name)\n",
        "        )\n",
        "        trainable_vars = self.bert.variables\n",
        "        \n",
        "        # Remove unused layers\n",
        "        trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
        "        \n",
        "        # Select how many layers to fine tune\n",
        "        trainable_vars = trainable_vars[-self.n_fine_tune_layers :]\n",
        "        \n",
        "        # Add to trainable weights\n",
        "        for var in trainable_vars:\n",
        "            self._trainable_weights.append(var)\n",
        "        \n",
        "        # Add non-trainable weights\n",
        "        for var in self.bert.variables:\n",
        "            if var not in self._trainable_weights:\n",
        "                self._non_trainable_weights.append(var)\n",
        "        \n",
        "        super(BertLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
        "        input_ids, input_mask, segment_ids = inputs\n",
        "        bert_inputs = dict(\n",
        "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
        "        )\n",
        "        result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
        "            \"pooled_output\"\n",
        "        ]\n",
        "        return result\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], self.output_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzvSfxJ2hMTL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "86078102-c90b-453c-d021-ac6e6a6b6f41"
      },
      "source": [
        "def create_model(n_fine_tune_layers):\n",
        "  # Build model\n",
        "  in_id = keras.layers.Input(shape=(MAX_SEQ_LENGTH,), name=\"input_ids\")\n",
        "  in_mask = keras.layers.Input(shape=(MAX_SEQ_LENGTH,), name=\"input_masks\")\n",
        "  in_segment = keras.layers.Input(shape=(MAX_SEQ_LENGTH,), name=\"segment_ids\")\n",
        "  bert_inputs = [in_id, in_mask, in_segment]\n",
        "\n",
        "  # Instantiate the custom Bert Layer defined above\n",
        "  bert_output = BertLayer(n_fine_tune_layers=n_fine_tune_layers)(bert_inputs)\n",
        "\n",
        "  # Build the rest of the classifier \n",
        "  dense = keras.layers.Dense(256, activation='relu')(bert_output)\n",
        "  drop = keras.layers.Dropout(0.5)(dense)\n",
        "  pred = keras.layers.Dense(len(LABELS), activation='softmax')(drop)\n",
        "  model = keras.models.Model(inputs=bert_inputs, outputs=pred) \n",
        "  \n",
        "  return model\n",
        "\n",
        "model = create_model(n_fine_tune_layers=0)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(\n",
        "      [train_input_ids, train_input_mask, train_segment_ids], \n",
        "      train_label_id,\n",
        "      validation_data=([valid_input_ids, valid_input_mask, valid_segment_ids], valid_label_id),\n",
        "      epochs=1,\n",
        "      batch_size=32\n",
        "  )"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0613 22:49:12.553112 140714811475840 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0613 22:49:12.558936 140714811475840 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0613 22:49:15.620188 140714811475840 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0613 22:49:15.637148 140714811475840 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0613 22:49:15.646409 140714811475840 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0613 22:49:15.694777 140714811475840 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0613 22:49:16.612426 140714811475840 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 18681 samples, validate on 9032 samples\n",
            "Epoch 1/1\n",
            "18681/18681 [==============================] - 578s 31ms/step - loss: 3.1970 - acc: 0.1555 - val_loss: 3.0403 - val_acc: 0.1784\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ffa01e14908>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IS7_n7vAwnUS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "690e6f22-75e3-4577-b964-aa9ff4e1a953"
      },
      "source": [
        "model.layers[4].n_fine_tune_layers=42\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(\n",
        "      [train_input_ids, train_input_mask, train_segment_ids], \n",
        "      train_label_id,\n",
        "      validation_data=([valid_input_ids, valid_input_mask, valid_segment_ids], valid_label_id),\n",
        "      epochs=10,\n",
        "      batch_size=32\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 18681 samples, validate on 9032 samples\n",
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oY1zQ4qG8PpF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCXg6qyH8Rml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b.trainable_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBBXOk1l8dMV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sroRnAC-yAJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.layers[4].n_fine_tune_layers"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}