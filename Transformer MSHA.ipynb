{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Recurrent Neural Networks.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ameasure/colab_tutorials/blob/master/Transformer%20MSHA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6H8_-9hWKd4",
        "colab_type": "text"
      },
      "source": [
        "# Recurrent Neural Networks\n",
        "\n",
        "We have seen that convolutional neural networks allow us to efficiently model local structure in our data, but this is not the only sort of structure that exists in nature. What else is there?\n",
        "\n",
        "### Sequential Structure\n",
        "Consider the process of reading a document and understanding what it means. Locality is certainly still relevant, words close to each other are more likely to relate to each other, but the relationships between the words are not strictly limited to any fixed window and the interactions between words are primarily one directional. Our understanding of the document at any point in time is informed primarily by the words that have been read up to that point, not to the possibly nearby words that have not yet been read. This is a sequential structure, a type of structure that is enforced by the seemingly one-directional passage of time. As such it frequently occurs in any process affected by the passage of time. Convolutions lack this one-directional bias. What sort of model could allow us to better reflect this bias, and perhaps relax the locality constraint in one direction in return? \n",
        "\n",
        "One option is a recurrent neural network. In its simplest form, a recurrent neural network is simply a single dense layer which is applied repeatedly to a sequence of inputs and uses as an additional input, its output from the previous step in the sequence. In theory, such a model can capture relationships between inputs separated by an infinite distance. Furthermore, because all information is processed in sequence the sequential bias is strictly enforced. An example of the operation of a recurrent neural network is illustrated below:\n",
        "\n",
        "![Images](https://github.com/ameasure/colab_tutorials/blob/master/Images/rnn_loop.gif?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IBuWbB2WKd6",
        "colab_type": "text"
      },
      "source": [
        "As with convolutional layers, it is often helpful to follow a recurrent layer with some sort of aggregation operation. Here, however, there is one additional option. We could just use the RNN's output from the very last step of the sequence. Since an RNN is theoretically capable of remembering all relevant information from the sequence this works. As a practical matter however, RNN's tend to forget information from distant inputs so its generally useful to use the intermediate outputs as well if those are of relevance. These can be captured through `mean` or `max pooling` (as we saw with convolutional neural networks), or through `attention` mechanisms which use additional layers to weight each output before averaging. \n",
        "\n",
        "### Preparing the Data\n",
        "\n",
        "We will prepare our data as we did with convolutional neural networks, by creating a sequence of vectors corresponding to the input words in our training narratives. We accomplish this with a combination of the Keras Tokenizer, which maps narrative to a sequence of numbers representing each word in the narrative, and then an Embedding layer which maps each index to a vector. This is equivalent to representing each input word with a 1-hot vector and then multiplying each of these 1-hot vectors by a Dense layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bmvq9xlZWKd7",
        "colab_type": "code",
        "outputId": "76a0ccad-2eb2-4c11-b561-c69f771077f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "# load the msha data file to Colab\n",
        "!wget 'https://github.com/ameasure/autocoding-class/raw/master/msha.xlsx'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-06-24 21:57:38--  https://github.com/ameasure/autocoding-class/raw/master/msha.xlsx\n",
            "Resolving github.com (github.com)... 52.74.223.119\n",
            "Connecting to github.com (github.com)|52.74.223.119|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/ameasure/autocoding-class/master/msha.xlsx [following]\n",
            "--2019-06-24 21:57:39--  https://raw.githubusercontent.com/ameasure/autocoding-class/master/msha.xlsx\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4183086 (4.0M) [application/octet-stream]\n",
            "Saving to: ‘msha.xlsx.1’\n",
            "\n",
            "msha.xlsx.1         100%[===================>]   3.99M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2019-06-24 21:57:39 (133 MB/s) - ‘msha.xlsx.1’ saved [4183086/4183086]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpuqevToWKd_",
        "colab_type": "code",
        "outputId": "06cfb16f-6b48-413b-b8ad-033703325505",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# read in and separate the training and validation data\n",
        "df = pd.read_excel('msha.xlsx')\n",
        "df['ACCIDENT_YEAR'] = df['ACCIDENT_DT'].apply(lambda x: x.year)\n",
        "df['ACCIDENT_YEAR'].value_counts()\n",
        "df_train = df[df['ACCIDENT_YEAR'].isin([2010, 2011])].copy()\n",
        "df_valid = df[df['ACCIDENT_YEAR'] == 2012].copy()\n",
        "print('training rows:', len(df_train))\n",
        "print('validation rows:', len(df_valid))\n",
        "\n",
        "# convert the narratives to sequences of indexes\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df_train['NARRATIVE'])\n",
        "X_train_seq = tokenizer.texts_to_sequences(df_train['NARRATIVE'])\n",
        "X_valid_seq = tokenizer.texts_to_sequences(df_valid['NARRATIVE'])\n",
        "\n",
        "# keras only accepts a one-hot encoding of the training labels\n",
        "# we do that here\n",
        "label_encoder = LabelBinarizer().fit(df_train['INJ_BODY_PART'])\n",
        "y_train = label_encoder.transform(df_train['INJ_BODY_PART'])\n",
        "y_valid = label_encoder.transform(df_valid['INJ_BODY_PART'])\n",
        "n_codes = len(label_encoder.classes_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training rows: 18681\n",
            "validation rows: 9032\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAG0IB93WKeB",
        "colab_type": "code",
        "outputId": "8d2b5932-2962-4fa1-fca9-2b248337f11b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(X_train_seq[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[244, 29, 7152, 1570, 764, 213, 970, 4, 3198, 139, 5, 1924, 424, 223, 610, 1, 764, 29, 10, 1, 1570, 9, 3, 64, 2, 490, 110, 5, 213, 1, 764, 813, 4, 164, 317, 11, 6, 15, 54]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "668E7DVWWKeJ",
        "colab_type": "text"
      },
      "source": [
        "Although an RNN can, in theory, work with sequences of arbitrary length, for computational reasons it is necessary to make sure that each batch of input examples has the same length. We accomplish this as we did with the convolutional neural networks, by padding (or truncating) all input sequences to the same length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aoqs_cukWKeJ",
        "colab_type": "code",
        "outputId": "1a70d78c-7969-41f0-afc0-bc86c539b219",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "from keras.preprocessing import sequence\n",
        "\n",
        "X_train_seq = sequence.pad_sequences(X_train_seq, maxlen=200)\n",
        "X_valid_seq = sequence.pad_sequences(X_valid_seq, maxlen=200)\n",
        "\n",
        "print(X_train_seq[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0  244   29 7152 1570  764  213  970\n",
            "    4 3198  139    5 1924  424  223  610    1  764   29   10    1 1570\n",
            "    9    3   64    2  490  110    5  213    1  764  813    4  164  317\n",
            "   11    6   15   54]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5nGq0cKWKeN",
        "colab_type": "text"
      },
      "source": [
        "We're now ready to specify the recurrent neural network. Here we use a particular type of recurrent neural network called an LSTM, which stands for Long-Short-Term-Memory. This contains a number of modifications which improve the performance. We will not go into the details in this class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5qKIOiXWKeO",
        "colab_type": "code",
        "outputId": "c731ce0a-d82f-4d0c-9344-a1dc364fae9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Dense, Input, Dropout\n",
        "from keras.layers import Embedding, LSTM, GlobalMaxPooling1D, Concatenate\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "input_text = Input(shape=(200,), dtype='int32')\n",
        "embedding = Embedding(input_dim=len(tokenizer.word_index), \n",
        "                      output_dim=300, \n",
        "                      input_length=200)(input_text)\n",
        "lstm = LSTM(units=256, \n",
        "            dropout=0.5, \n",
        "            recurrent_dropout=0.5, \n",
        "            return_sequences=True, )(embedding)\n",
        "pool = GlobalMaxPooling1D()(lstm)\n",
        "dropout = Dropout(0.5)(pool)\n",
        "output = Dense(len(label_encoder.classes_), activation='softmax')(dropout)\n",
        "\n",
        "model = Model(inputs=input_text, outputs=output)\n",
        "optimer = Adam(lr=.001)\n",
        "model.compile(optimizer='adam', \n",
        "              loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0624 21:57:46.684967 140084453189504 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0624 21:57:46.698711 140084453189504 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0624 21:57:46.702325 140084453189504 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0624 21:57:46.891983 140084453189504 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0624 21:57:46.905144 140084453189504 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0624 21:57:47.392223 140084453189504 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0624 21:57:47.415475 140084453189504 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUz3gxgdWKeQ",
        "colab_type": "code",
        "outputId": "85654f7d-15a1-4943-a813-cc0ab48e187d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 793
        }
      },
      "source": [
        "model.fit(x=X_train_seq, y=y_train,\n",
        "          validation_data=(X_valid_seq, y_valid),\n",
        "          batch_size=512, epochs=20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0624 21:57:47.525758 140084453189504 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 18681 samples, validate on 9032 samples\n",
            "Epoch 1/20\n",
            "18681/18681 [==============================] - 18s 979us/step - loss: 3.3031 - acc: 0.1407 - val_loss: 3.0453 - val_acc: 0.1784\n",
            "Epoch 2/20\n",
            "18681/18681 [==============================] - 16s 867us/step - loss: 3.0943 - acc: 0.1771 - val_loss: 2.8794 - val_acc: 0.2542\n",
            "Epoch 3/20\n",
            "18681/18681 [==============================] - 16s 851us/step - loss: 2.7074 - acc: 0.3312 - val_loss: 2.2349 - val_acc: 0.4864\n",
            "Epoch 4/20\n",
            "18681/18681 [==============================] - 16s 852us/step - loss: 2.1240 - acc: 0.4880 - val_loss: 1.7296 - val_acc: 0.5870\n",
            "Epoch 5/20\n",
            "18681/18681 [==============================] - 16s 852us/step - loss: 1.7309 - acc: 0.5827 - val_loss: 1.4404 - val_acc: 0.6538\n",
            "Epoch 6/20\n",
            "18681/18681 [==============================] - 16s 856us/step - loss: 1.4771 - acc: 0.6441 - val_loss: 1.2349 - val_acc: 0.7097\n",
            "Epoch 7/20\n",
            "18681/18681 [==============================] - 16s 849us/step - loss: 1.2713 - acc: 0.6911 - val_loss: 1.0937 - val_acc: 0.7314\n",
            "Epoch 8/20\n",
            "18681/18681 [==============================] - 16s 849us/step - loss: 1.1276 - acc: 0.7220 - val_loss: 0.9931 - val_acc: 0.7522\n",
            "Epoch 9/20\n",
            "18681/18681 [==============================] - 16s 852us/step - loss: 1.0160 - acc: 0.7440 - val_loss: 0.9370 - val_acc: 0.7615\n",
            "Epoch 10/20\n",
            "18681/18681 [==============================] - 16s 852us/step - loss: 0.9219 - acc: 0.7621 - val_loss: 0.8971 - val_acc: 0.7671\n",
            "Epoch 11/20\n",
            "18681/18681 [==============================] - 16s 846us/step - loss: 0.8465 - acc: 0.7787 - val_loss: 0.8737 - val_acc: 0.7715\n",
            "Epoch 12/20\n",
            "18681/18681 [==============================] - 16s 846us/step - loss: 0.7972 - acc: 0.7932 - val_loss: 0.8549 - val_acc: 0.7740\n",
            "Epoch 13/20\n",
            "18681/18681 [==============================] - 16s 841us/step - loss: 0.7437 - acc: 0.8025 - val_loss: 0.8438 - val_acc: 0.7780\n",
            "Epoch 14/20\n",
            "18681/18681 [==============================] - 16s 851us/step - loss: 0.7035 - acc: 0.8128 - val_loss: 0.8376 - val_acc: 0.7798\n",
            "Epoch 15/20\n",
            "18681/18681 [==============================] - 16s 855us/step - loss: 0.6636 - acc: 0.8213 - val_loss: 0.8291 - val_acc: 0.7807\n",
            "Epoch 16/20\n",
            "18681/18681 [==============================] - 16s 848us/step - loss: 0.6286 - acc: 0.8314 - val_loss: 0.8360 - val_acc: 0.7788\n",
            "Epoch 17/20\n",
            "18681/18681 [==============================] - 16s 849us/step - loss: 0.6024 - acc: 0.8379 - val_loss: 0.8299 - val_acc: 0.7819\n",
            "Epoch 18/20\n",
            "18681/18681 [==============================] - 16s 847us/step - loss: 0.5702 - acc: 0.8464 - val_loss: 0.8318 - val_acc: 0.7829\n",
            "Epoch 19/20\n",
            "18681/18681 [==============================] - 16s 851us/step - loss: 0.5497 - acc: 0.8511 - val_loss: 0.8261 - val_acc: 0.7840\n",
            "Epoch 20/20\n",
            "18681/18681 [==============================] - 16s 851us/step - loss: 0.5198 - acc: 0.8592 - val_loss: 0.8338 - val_acc: 0.7847\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f67a0b23128>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgCSE_H0WKeU",
        "colab_type": "text"
      },
      "source": [
        "A popular trick that sometimes results in some additional performance is to add another recurrent layer operating in the reverse direction of the sequence. We can implement this in Keras by wrapping our RNN with a Bidirectional layer. It simply creates two LSTM layers, one running left-to-right, the other right-to-left, and concatenates their outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5F7IYIXHWKeV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Bidirectional\n",
        "\n",
        "\n",
        "input_text = Input(shape=(200,), dtype='int32')\n",
        "embedding = Embedding(input_dim=len(tokenizer.word_index), \n",
        "                      output_dim=300, \n",
        "                      input_length=200)(input_text)\n",
        "lstm = Bidirectional(LSTM(units=128, \n",
        "                          dropout=0.5, \n",
        "                          recurrent_dropout=0.5, \n",
        "                          return_sequences=True),\n",
        "                     merge_mode='concat')(embedding)\n",
        "pool = GlobalMaxPooling1D()(lstm)\n",
        "dropout = Dropout(0.5)(pool)\n",
        "output = Dense(len(label_encoder.classes_), activation='softmax')(dropout)\n",
        "\n",
        "model = Model(inputs=input_text, outputs=output)\n",
        "optimer = Adam(lr=.001)\n",
        "model.compile(optimizer='adam', \n",
        "              loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaClLUtRWKeY",
        "colab_type": "code",
        "outputId": "f2a6e38a-6128-4b9f-ad8f-ce4f19e0924d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        }
      },
      "source": [
        "model.fit(x=X_train_seq, y=y_train,\n",
        "          validation_data=(X_valid_seq, y_valid),\n",
        "          batch_size=512, epochs=20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 18681 samples, validate on 9032 samples\n",
            "Epoch 1/20\n",
            "18681/18681 [==============================] - 34s 2ms/step - loss: 3.3131 - acc: 0.1525 - val_loss: 3.0118 - val_acc: 0.1784\n",
            "Epoch 2/20\n",
            "18681/18681 [==============================] - 32s 2ms/step - loss: 2.9045 - acc: 0.2619 - val_loss: 2.3839 - val_acc: 0.3895\n",
            "Epoch 3/20\n",
            "18681/18681 [==============================] - 33s 2ms/step - loss: 2.1516 - acc: 0.4781 - val_loss: 1.7205 - val_acc: 0.5736\n",
            "Epoch 4/20\n",
            "18681/18681 [==============================] - 32s 2ms/step - loss: 1.6972 - acc: 0.5794 - val_loss: 1.4461 - val_acc: 0.6372\n",
            "Epoch 5/20\n",
            "18681/18681 [==============================] - 33s 2ms/step - loss: 1.4509 - acc: 0.6403 - val_loss: 1.2416 - val_acc: 0.6898\n",
            "Epoch 6/20\n",
            "18681/18681 [==============================] - 32s 2ms/step - loss: 1.2345 - acc: 0.6873 - val_loss: 1.0891 - val_acc: 0.7200\n",
            "Epoch 7/20\n",
            "18681/18681 [==============================] - 32s 2ms/step - loss: 1.0757 - acc: 0.7266 - val_loss: 0.9839 - val_acc: 0.7481\n",
            "Epoch 8/20\n",
            "18681/18681 [==============================] - 32s 2ms/step - loss: 0.9653 - acc: 0.7515 - val_loss: 0.9169 - val_acc: 0.7632\n",
            "Epoch 9/20\n",
            "18681/18681 [==============================] - 32s 2ms/step - loss: 0.8654 - acc: 0.7744 - val_loss: 0.8714 - val_acc: 0.7704\n",
            "Epoch 10/20\n",
            "18681/18681 [==============================] - 33s 2ms/step - loss: 0.7978 - acc: 0.7900 - val_loss: 0.8454 - val_acc: 0.7749\n",
            "Epoch 11/20\n",
            "18681/18681 [==============================] - 32s 2ms/step - loss: 0.7297 - acc: 0.8055 - val_loss: 0.8283 - val_acc: 0.7808\n",
            "Epoch 12/20\n",
            "18681/18681 [==============================] - 32s 2ms/step - loss: 0.6786 - acc: 0.8193 - val_loss: 0.8150 - val_acc: 0.7839\n",
            "Epoch 13/20\n",
            "18681/18681 [==============================] - 32s 2ms/step - loss: 0.6323 - acc: 0.8331 - val_loss: 0.8111 - val_acc: 0.7890\n",
            "Epoch 14/20\n",
            "18681/18681 [==============================] - 32s 2ms/step - loss: 0.5888 - acc: 0.8414 - val_loss: 0.8094 - val_acc: 0.7884\n",
            "Epoch 15/20\n",
            "18681/18681 [==============================] - 33s 2ms/step - loss: 0.5539 - acc: 0.8504 - val_loss: 0.8142 - val_acc: 0.7899\n",
            "Epoch 16/20\n",
            "18681/18681 [==============================] - 32s 2ms/step - loss: 0.5171 - acc: 0.8602 - val_loss: 0.8195 - val_acc: 0.7902\n",
            "Epoch 17/20\n",
            "18681/18681 [==============================] - 32s 2ms/step - loss: 0.4918 - acc: 0.8667 - val_loss: 0.8270 - val_acc: 0.7916\n",
            "Epoch 18/20\n",
            "18681/18681 [==============================] - 32s 2ms/step - loss: 0.4690 - acc: 0.8716 - val_loss: 0.8406 - val_acc: 0.7897\n",
            "Epoch 19/20\n",
            "18681/18681 [==============================] - 32s 2ms/step - loss: 0.4395 - acc: 0.8804 - val_loss: 0.8474 - val_acc: 0.7890\n",
            "Epoch 20/20\n",
            "18681/18681 [==============================] - 32s 2ms/step - loss: 0.4102 - acc: 0.8882 - val_loss: 0.8622 - val_acc: 0.7872\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f67a14e5208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SI1v_7mlAcLj",
        "colab_type": "text"
      },
      "source": [
        "# Next Lesson\n",
        "[Pretrained Language Models](https://colab.research.google.com/drive/12wYVNlqC2U_7O07m4iT0R55ug9TSO6wn)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcqZSPx7WKeb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}