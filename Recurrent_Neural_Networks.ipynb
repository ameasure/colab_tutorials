{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Recurrent Neural Networks.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ameasure/colab_tutorials/blob/master/Recurrent_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6H8_-9hWKd4",
        "colab_type": "text"
      },
      "source": [
        "# Recurrent Neural Networks\n",
        "\n",
        "We have seen that convolutional neural networks allow us to efficiently model local structure in our data, but this is not the only sort of structure that exists in nature. What else is there?\n",
        "\n",
        "### Sequential Structure\n",
        "Consider the process of reading a document and understanding what it means. Locality is certainly still relevant, words close to each other are more likely to relate to each other, but the relationships between the words are not strictly limited to any fixed window and the interactions between words are primarily one directional. Our understanding of the document at any point in time is informed primarily by the words that have been read up to that point, not to the possibly nearby words that have not yet been read. This is a sequential structure, a type of structure that is enforced by the seemingly one-directional passage of time. As such it frequently occurs in any process affected by the passage of time. Convolutions lack this one-directional bias. What sort of model could allow us to better reflect this bias, and perhaps relax the locality constraint in one direction in return? \n",
        "\n",
        "One option is a recurrent neural network. In its simplest form, a recurrent neural network is simply a single dense layer which is applied repeatedly to a sequence of inputs and uses as an additional input, its output from the previous step in the sequence. In theory, such a model can capture relationships between inputs separated by an infinite distance. Furthermore, because all information is processed in sequence the sequential bias is strictly enforced. An example of the operation of a recurrent neural network is illustrated below:\n",
        "\n",
        "![Images](https://github.com/ameasure/colab_tutorials/blob/master/Images/rnn_loop.gif?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IBuWbB2WKd6",
        "colab_type": "text"
      },
      "source": [
        "As with convolutional layers, it is often helpful to follow a recurrent layer with some sort of aggregation operation. Here, however, there is one additional option. We could just use the RNN's output from the very last step of the sequence. Since an RNN is theoretically capable of remembering all relevant information from the sequence this works. As a practical matter however, RNN's tend to forget information from distant inputs so its generally useful to use the intermediate outputs as well if those are of relevance. These can be captured through `mean` or `max pooling` (as we saw with convolutional neural networks), or through `attention` mechanisms which use additional layers to weight each output before averaging. \n",
        "\n",
        "### Preparing the Data\n",
        "\n",
        "We will prepare our data as we did with convolutional neural networks, by creating a sequence of vectors corresponding to the input words in our training narratives. We accomplish this with a combination of the Keras Tokenizer, which maps narrative to a sequence of numbers representing each word in the narrative, and then an Embedding layer which maps each index to a vector. This is equivalent to representing each input word with a 1-hot vector and then multiplying each of these 1-hot vectors by a Dense layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bmvq9xlZWKd7",
        "colab_type": "code",
        "outputId": "a7eff7ed-86ce-42af-8f7e-5902cbf27910",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "source": [
        "# load the msha data file to Colab\n",
        "!wget 'https://github.com/ameasure/autocoding-class/raw/master/msha.xlsx'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-03-22 17:11:50--  https://github.com/ameasure/autocoding-class/raw/master/msha.xlsx\n",
            "Resolving github.com (github.com)... 192.30.253.112, 192.30.253.113\n",
            "Connecting to github.com (github.com)|192.30.253.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/ameasure/autocoding-class/master/msha.xlsx [following]\n",
            "--2019-03-22 17:11:50--  https://raw.githubusercontent.com/ameasure/autocoding-class/master/msha.xlsx\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4183086 (4.0M) [application/octet-stream]\n",
            "Saving to: ‘msha.xlsx.1’\n",
            "\n",
            "msha.xlsx.1         100%[===================>]   3.99M  21.1MB/s    in 0.2s    \n",
            "\n",
            "2019-03-22 17:11:51 (21.1 MB/s) - ‘msha.xlsx.1’ saved [4183086/4183086]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpuqevToWKd_",
        "colab_type": "code",
        "outputId": "f729f602-047d-41c4-f9ec-b490f4203b6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# read in and separate the training and validation data\n",
        "df = pd.read_excel('msha.xlsx')\n",
        "df['ACCIDENT_YEAR'] = df['ACCIDENT_DT'].apply(lambda x: x.year)\n",
        "df['ACCIDENT_YEAR'].value_counts()\n",
        "df_train = df[df['ACCIDENT_YEAR'].isin([2010, 2011])].copy()\n",
        "df_valid = df[df['ACCIDENT_YEAR'] == 2012].copy()\n",
        "print('training rows:', len(df_train))\n",
        "print('validation rows:', len(df_valid))\n",
        "\n",
        "# convert the narratives to sequences of indexes\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df_train['NARRATIVE'])\n",
        "X_train_seq = tokenizer.texts_to_sequences(df_train['NARRATIVE'])\n",
        "X_valid_seq = tokenizer.texts_to_sequences(df_valid['NARRATIVE'])\n",
        "\n",
        "# keras only accepts a one-hot encoding of the training labels\n",
        "# we do that here\n",
        "label_encoder = LabelBinarizer().fit(df_train['INJ_BODY_PART'])\n",
        "y_train = label_encoder.transform(df_train['INJ_BODY_PART'])\n",
        "y_valid = label_encoder.transform(df_valid['INJ_BODY_PART'])\n",
        "n_codes = len(label_encoder.classes_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training rows: 18681\n",
            "validation rows: 9032\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAG0IB93WKeB",
        "colab_type": "code",
        "outputId": "e77cab3a-5cc7-4b98-e485-878584ecacc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(X_train_seq[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[244, 29, 7152, 1570, 764, 213, 970, 4, 3198, 139, 5, 1924, 424, 223, 610, 1, 764, 29, 10, 1, 1570, 9, 3, 64, 2, 490, 110, 5, 213, 1, 764, 813, 4, 164, 317, 11, 6, 15, 54]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "668E7DVWWKeJ",
        "colab_type": "text"
      },
      "source": [
        "Although an RNN can, in theory, work with sequences of arbitrary length, for computational reasons it is necessary to make sure that each batch of input examples has the same length. We accomplish this as we did with the convolutional neural networks, by padding (or truncating) all input sequences to the same length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aoqs_cukWKeJ",
        "colab_type": "code",
        "outputId": "f0493555-3c4f-44e3-b378-b9c13b6a8642",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        }
      },
      "source": [
        "from keras.preprocessing import sequence\n",
        "\n",
        "X_train_seq = sequence.pad_sequences(X_train_seq, maxlen=200)\n",
        "X_valid_seq = sequence.pad_sequences(X_valid_seq, maxlen=200)\n",
        "\n",
        "print(X_train_seq[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0  244   29 7152 1570  764  213  970\n",
            "    4 3198  139    5 1924  424  223  610    1  764   29   10    1 1570\n",
            "    9    3   64    2  490  110    5  213    1  764  813    4  164  317\n",
            "   11    6   15   54]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5nGq0cKWKeN",
        "colab_type": "text"
      },
      "source": [
        "We're now ready to specify the recurrent neural network. Here we use a particular type of recurrent neural network called an LSTM, which stands for Long-Short-Term-Memory. This contains a number of modifications which improve the performance. We will not go into the details in this class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5qKIOiXWKeO",
        "colab_type": "code",
        "outputId": "c203603e-bda3-4613-823b-3edfa2827f87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Dense, Input, Dropout\n",
        "from keras.layers import Embedding, LSTM, GlobalMaxPooling1D, Concatenate\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "input_text = Input(shape=(200,), dtype='int32')\n",
        "embedding = Embedding(input_dim=len(tokenizer.word_index), \n",
        "                      output_dim=300, \n",
        "                      input_length=200)(input_text)\n",
        "lstm = LSTM(units=256, \n",
        "            dropout=0.5, \n",
        "            recurrent_dropout=0.5, \n",
        "            return_sequences=True, )(embedding)\n",
        "pool = GlobalMaxPooling1D()(lstm)\n",
        "dropout = Dropout(0.5)(pool)\n",
        "output = Dense(len(label_encoder.classes_), activation='softmax')(dropout)\n",
        "\n",
        "model = Model(inputs=input_text, outputs=output)\n",
        "optimer = Adam(lr=.001)\n",
        "model.compile(optimizer='adam', \n",
        "              loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUz3gxgdWKeQ",
        "colab_type": "code",
        "outputId": "8b7b5255-cee8-4c87-86eb-15f748e09610",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "model.fit(x=X_train_seq, y=y_train,\n",
        "          validation_data=(X_valid_seq, y_valid),\n",
        "          batch_size=512, epochs=20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "Train on 18681 samples, validate on 9032 samples\n",
            "Epoch 1/20\n",
            "18681/18681 [==============================] - 34s 2ms/step - loss: 3.2958 - acc: 0.1429 - val_loss: 3.0411 - val_acc: 0.1784\n",
            "Epoch 2/20\n",
            "18681/18681 [==============================] - 31s 2ms/step - loss: 3.0833 - acc: 0.1822 - val_loss: 2.8502 - val_acc: 0.2766\n",
            "Epoch 3/20\n",
            "18681/18681 [==============================] - 31s 2ms/step - loss: 2.6521 - acc: 0.3375 - val_loss: 2.1681 - val_acc: 0.4909\n",
            "Epoch 4/20\n",
            "18681/18681 [==============================] - 31s 2ms/step - loss: 2.0516 - acc: 0.5076 - val_loss: 1.6661 - val_acc: 0.6026\n",
            "Epoch 5/20\n",
            "18681/18681 [==============================] - 31s 2ms/step - loss: 1.6447 - acc: 0.5983 - val_loss: 1.3514 - val_acc: 0.6821\n",
            "Epoch 6/20\n",
            "18681/18681 [==============================] - 31s 2ms/step - loss: 1.3684 - acc: 0.6656 - val_loss: 1.1390 - val_acc: 0.7201\n",
            "Epoch 7/20\n",
            "18681/18681 [==============================] - 31s 2ms/step - loss: 1.1717 - acc: 0.7110 - val_loss: 1.0091 - val_acc: 0.7483\n",
            "Epoch 8/20\n",
            "18681/18681 [==============================] - 31s 2ms/step - loss: 1.0348 - acc: 0.7428 - val_loss: 0.9216 - val_acc: 0.7668\n",
            "Epoch 9/20\n",
            "18681/18681 [==============================] - 31s 2ms/step - loss: 0.9168 - acc: 0.7675 - val_loss: 0.8669 - val_acc: 0.7748\n",
            "Epoch 10/20\n",
            "18681/18681 [==============================] - 31s 2ms/step - loss: 0.8314 - acc: 0.7861 - val_loss: 0.8305 - val_acc: 0.7838\n",
            "Epoch 11/20\n",
            "18681/18681 [==============================] - 31s 2ms/step - loss: 0.7618 - acc: 0.8040 - val_loss: 0.8123 - val_acc: 0.7869\n",
            "Epoch 12/20\n",
            "18681/18681 [==============================] - 31s 2ms/step - loss: 0.7063 - acc: 0.8128 - val_loss: 0.8024 - val_acc: 0.7885\n",
            "Epoch 13/20\n",
            "18681/18681 [==============================] - 31s 2ms/step - loss: 0.6614 - acc: 0.8231 - val_loss: 0.8001 - val_acc: 0.7896\n",
            "Epoch 14/20\n",
            "18681/18681 [==============================] - 31s 2ms/step - loss: 0.6059 - acc: 0.8359 - val_loss: 0.7996 - val_acc: 0.7888\n",
            "Epoch 15/20\n",
            "18681/18681 [==============================] - 31s 2ms/step - loss: 0.5772 - acc: 0.8448 - val_loss: 0.8039 - val_acc: 0.7890\n",
            "Epoch 16/20\n",
            "18681/18681 [==============================] - 31s 2ms/step - loss: 0.5421 - acc: 0.8558 - val_loss: 0.8035 - val_acc: 0.7912\n",
            "Epoch 17/20\n",
            "18681/18681 [==============================] - 31s 2ms/step - loss: 0.5196 - acc: 0.8569 - val_loss: 0.8165 - val_acc: 0.7900\n",
            "Epoch 18/20\n",
            "18681/18681 [==============================] - 31s 2ms/step - loss: 0.4916 - acc: 0.8676 - val_loss: 0.8145 - val_acc: 0.7907\n",
            "Epoch 19/20\n",
            "18681/18681 [==============================] - 31s 2ms/step - loss: 0.4696 - acc: 0.8736 - val_loss: 0.8218 - val_acc: 0.7897\n",
            "Epoch 20/20\n",
            "18681/18681 [==============================] - 31s 2ms/step - loss: 0.4485 - acc: 0.8785 - val_loss: 0.8432 - val_acc: 0.7857\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6d3f388dd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgCSE_H0WKeU",
        "colab_type": "text"
      },
      "source": [
        "A popular trick that sometimes results in some additional performance is to add another recurrent layer operating in the reverse direction of the sequence. We can implement this in Keras by wrapping our RNN with a Bidirectional layer. It simply creates two LSTM layers, one running left-to-right, the other right-to-left, and concatenates their outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5F7IYIXHWKeV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Bidirectional\n",
        "\n",
        "\n",
        "input_text = Input(shape=(200,), dtype='int32')\n",
        "embedding = Embedding(input_dim=len(tokenizer.word_index), \n",
        "                      output_dim=300, \n",
        "                      input_length=200)(input_text)\n",
        "lstm = Bidirectional(LSTM(units=128, \n",
        "                          dropout=0.5, \n",
        "                          recurrent_dropout=0.5, \n",
        "                          return_sequences=True),\n",
        "                     merge_mode='concat')(embedding)\n",
        "pool = GlobalMaxPooling1D()(lstm)\n",
        "dropout = Dropout(0.5)(pool)\n",
        "output = Dense(len(label_encoder.classes_), activation='softmax')(dropout)\n",
        "\n",
        "model = Model(inputs=input_text, outputs=output)\n",
        "optimer = Adam(lr=.001)\n",
        "model.compile(optimizer='adam', \n",
        "              loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaClLUtRWKeY",
        "colab_type": "code",
        "outputId": "bc8a2b92-2f5f-4897-a902-f9fab5dc1fa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 743
        }
      },
      "source": [
        "model.fit(x=X_train_seq, y=y_train,\n",
        "          validation_data=(X_valid_seq, y_valid),\n",
        "          batch_size=512, epochs=20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 18681 samples, validate on 9032 samples\n",
            "Epoch 1/20\n",
            "18681/18681 [==============================] - 51s 3ms/step - loss: 3.3117 - acc: 0.1491 - val_loss: 3.0196 - val_acc: 0.1784\n",
            "Epoch 2/20\n",
            "18681/18681 [==============================] - 47s 3ms/step - loss: 2.9610 - acc: 0.2372 - val_loss: 2.5004 - val_acc: 0.3397\n",
            "Epoch 3/20\n",
            "18681/18681 [==============================] - 48s 3ms/step - loss: 2.1860 - acc: 0.4603 - val_loss: 1.7411 - val_acc: 0.5626\n",
            "Epoch 4/20\n",
            "18681/18681 [==============================] - 48s 3ms/step - loss: 1.7217 - acc: 0.5674 - val_loss: 1.4947 - val_acc: 0.6153\n",
            "Epoch 5/20\n",
            "18681/18681 [==============================] - 48s 3ms/step - loss: 1.4897 - acc: 0.6242 - val_loss: 1.2962 - val_acc: 0.6883\n",
            "Epoch 6/20\n",
            "18681/18681 [==============================] - 48s 3ms/step - loss: 1.2784 - acc: 0.6902 - val_loss: 1.1147 - val_acc: 0.7321\n",
            "Epoch 7/20\n",
            "18681/18681 [==============================] - 48s 3ms/step - loss: 1.0940 - acc: 0.7297 - val_loss: 0.9930 - val_acc: 0.7469\n",
            "Epoch 8/20\n",
            "18681/18681 [==============================] - 48s 3ms/step - loss: 0.9706 - acc: 0.7522 - val_loss: 0.9203 - val_acc: 0.7630\n",
            "Epoch 9/20\n",
            "18681/18681 [==============================] - 47s 3ms/step - loss: 0.8697 - acc: 0.7747 - val_loss: 0.8753 - val_acc: 0.7685\n",
            "Epoch 10/20\n",
            "18681/18681 [==============================] - 47s 3ms/step - loss: 0.7927 - acc: 0.7906 - val_loss: 0.8402 - val_acc: 0.7781\n",
            "Epoch 11/20\n",
            "18681/18681 [==============================] - 47s 3ms/step - loss: 0.7317 - acc: 0.8075 - val_loss: 0.8223 - val_acc: 0.7820\n",
            "Epoch 12/20\n",
            "18681/18681 [==============================] - 48s 3ms/step - loss: 0.6811 - acc: 0.8207 - val_loss: 0.8176 - val_acc: 0.7827\n",
            "Epoch 13/20\n",
            "18681/18681 [==============================] - 47s 3ms/step - loss: 0.6318 - acc: 0.8319 - val_loss: 0.8073 - val_acc: 0.7837\n",
            "Epoch 14/20\n",
            "18681/18681 [==============================] - 47s 3ms/step - loss: 0.5983 - acc: 0.8395 - val_loss: 0.8071 - val_acc: 0.7879\n",
            "Epoch 15/20\n",
            "18681/18681 [==============================] - 47s 3ms/step - loss: 0.5578 - acc: 0.8477 - val_loss: 0.8107 - val_acc: 0.7870\n",
            "Epoch 16/20\n",
            "18681/18681 [==============================] - 47s 2ms/step - loss: 0.5265 - acc: 0.8579 - val_loss: 0.8158 - val_acc: 0.7878\n",
            "Epoch 17/20\n",
            "18681/18681 [==============================] - 46s 2ms/step - loss: 0.4937 - acc: 0.8653 - val_loss: 0.8263 - val_acc: 0.7874\n",
            "Epoch 18/20\n",
            "18681/18681 [==============================] - 47s 2ms/step - loss: 0.4746 - acc: 0.8723 - val_loss: 0.8350 - val_acc: 0.7874\n",
            "Epoch 19/20\n",
            "18681/18681 [==============================] - 47s 3ms/step - loss: 0.4515 - acc: 0.8773 - val_loss: 0.8378 - val_acc: 0.7891\n",
            "Epoch 20/20\n",
            "18681/18681 [==============================] - 47s 3ms/step - loss: 0.4199 - acc: 0.8861 - val_loss: 0.8433 - val_acc: 0.7895\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6d3115ee80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SI1v_7mlAcLj",
        "colab_type": "text"
      },
      "source": [
        "# Next Lesson\n",
        "[Pretrained Language Models](https://colab.research.google.com/drive/12wYVNlqC2U_7O07m4iT0R55ug9TSO6wn)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcqZSPx7WKeb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}