{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word Embeddings with Gensim.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ameasure/colab_tutorials/blob/master/Word_Embeddings_with_Gensim.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdNwsmP2NJLb",
        "colab_type": "text"
      },
      "source": [
        "# Self Supervised Learning\n",
        "Self supervised learning is a type of supervised learning in which the supervision comes from the input data itself rather than external labels that have been added. A common example is `language modeling`, in which we train a model to predict a hidden word from the words around it. Self supervised learning is a very important concept for two reasons:\n",
        "1. Essentially any data can be converted into self-supervised data by masking or corrupting part of the input and using the rest of the data to predict what is missing.\n",
        "2. By pretraining models on self-supervised tasks we can often dramatically reduce the amount of labeled training data needed to get good performance on supervised tasks. \n",
        "\n",
        "There are good reasons to suspect that much of human knowledge comes from self-supervised learning. After all, human learning occurs even in the absence of direct supervision and there is little doubt the brain is constantly anticipating, imagining, and filling in missing information (for example, there is an optic nerve blocking your field of vision right now and I bet you can't even see it).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "collapsed": true,
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": false,
        "id": "6EtNC-2WNJLj",
        "colab_type": "text"
      },
      "source": [
        "# Word Embeddings\n",
        "Word embeddings (i.e. dense vector representations of words) are currently one of the most popular uses of self-supervised learning (although they can be generated with direct supervision as well).  One of the motivations for creating word embeddings is to create word representations that capture the shared meaning between words, so that our NLP systems can recognize similarities between sentences even when no words are directly shared. Word embeddings are typically generated using some variation of the `language modeling` objective. One of the simplest and most efficient algorithms for training these is `word2vec`. There are several variants, but each essentially amounts to the following:\n",
        "1. sample words\n",
        "2. sample word contexts (surrounding words)\n",
        "3. predict one from the other\n",
        "\n",
        "We will demonstrate how to train these on our MSHA dataset using the `gensim` library. If you don't have it already you can install it from the Anaconda command prompt with `pip install gensim`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVXkDPQXMWPX",
        "colab_type": "text"
      },
      "source": [
        "# Load the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "IwdltEu9NJLg",
        "colab_type": "code",
        "outputId": "c2b7927f-0bef-45af-cafe-4d526202afb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# read in and separate the training and validation data\n",
        "df = pd.read_excel(r'Data/msha_2003-2018.xlsx')\n",
        "df['NARRATIVE'].fillna('', inplace=True)\n",
        "df['INJ_BODY_PART'].fillna('', inplace=True)\n",
        "df_train = df[df['YEAR'] < 2017].copy()\n",
        "df_valid = df[df['YEAR'] == 2018].copy()\n",
        "print('training rows:', len(df_train))\n",
        "print('validation rows:', len(df_valid))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training rows: 163352\n",
            "validation rows: 6778\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9D1orGGdNJLk",
        "colab_type": "text"
      },
      "source": [
        "# Step 1: Tokenize the Data\n",
        "\n",
        "Gensim expects language to be tokenized (broken into individual words) before being fed into the word2vec algorithm. We will do this with spacy as follows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "xFXCjRGJNJLk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "from spacy.lang.en import English\n",
        "\n",
        "# we disable all the annotators except the tokenizer so its fast\n",
        "nlp = English(disable=['tagger', 'parser', 'ner'])\n",
        "\n",
        "def tokenize(text):\n",
        "  return [t.text.lower() for t in nlp(text)]  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brjl6gQ5Ua5l",
        "colab_type": "code",
        "outputId": "7c641534-c841-4652-bd51-075ab1a12b5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "df_train['TOKENS'] = df_train['NARRATIVE'].apply(tokenize)\n",
        "df_train['TOKENS'].head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [using, drill, press, in, shop, to, drill, hol...\n",
              "1    [railroad, switch, was, iced, over, ., ee, jum...\n",
              "2    [ee, was, lifting, 5, gallon, buckets, of, med...\n",
              "3    [employee, was, working, on, sifter, blower, ....\n",
              "4    [employee, was, stepping, over, berm, to, get,...\n",
              "Name: TOKENS, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iNMG_KXMpco",
        "colab_type": "text"
      },
      "source": [
        "# Step 2: Fit the Word2Vec Model\n",
        "This is essentially a simple neural network where the inputs are target words and the outputs are context words (or vice versa, depending on the word2vec variant). The word embeddings are the activation formed by the hidden layer of the neural network when that word is presented as an input. The following are the main parameters of the model:\n",
        "1. `sentences` - a list of the tokenized texts we will use\n",
        "2. `size` - the dimensionality of the word embedding (100 means each word is mapped to a 100 element vector). 300 seems to be the most popular choice for embeddings trained on massive datasets.\n",
        "3. `window` - the distance in number of words considered \"in context\" for a given target word. Larger windows (>5) result in embeddings that are more reflective of word meaning. Smaller windows result in embeddings that are more reflective of word syntax (how it is used in a sentence). For example, \"good\" and \"bad\" have opposite meaning but similar syntax, i.e. you can replace one with the other in most sentences without violating any grammatical rules.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "1ryeOx_BNJLm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "w2vmodel = Word2Vec(sentences=df_train['TOKENS'], size=100, window=15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZv56cMANJLp",
        "colab_type": "text"
      },
      "source": [
        "Word vectors are designed to capture similarity in meaning between words. For example, we can see the words most similar to knee:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTjFBPjRPa1c",
        "colab_type": "text"
      },
      "source": [
        "# Step 3: Use the Embeddings\n",
        "\n",
        "Gensim makes it very easy to use the embeddings for a variety of purposes. We illustrate some below:\n",
        "\n",
        "### Find similar words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "KycsQDReNJLq",
        "colab_type": "code",
        "outputId": "919688e9-c6c4-4f83-e39e-12ad223629f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "w2vmodel.wv.most_similar('knee')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('ankle', 0.8319008946418762),\n",
              " ('leg', 0.6941453218460083),\n",
              " ('hip', 0.6907269358634949),\n",
              " ('elbow', 0.6734733581542969),\n",
              " ('shoulder', 0.6668279767036438),\n",
              " ('foot', 0.656676173210144),\n",
              " ('shin', 0.6550939083099365),\n",
              " ('calf', 0.6223568916320801),\n",
              " ('thigh', 0.5957974195480347),\n",
              " ('heel', 0.5956833362579346)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zubBMGdUQZBx",
        "colab_type": "text"
      },
      "source": [
        "### Calculate similarity between words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p31FmTbFQefZ",
        "colab_type": "code",
        "outputId": "6b1dfe1f-f8b0-41ca-837b-f0c6652296ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "w2vmodel.wv.similarity('contusions', 'bruises')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9211543"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2liKDi8RMM4",
        "colab_type": "code",
        "outputId": "8b0df240-d4d5-4bbb-a8d8-5a75782afc55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "w2vmodel.wv.similarity('employee', 'bruises')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.15032719"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BkbbztuR_-n",
        "colab_type": "text"
      },
      "source": [
        "### Similarity between sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFSyiNpiSIEM",
        "colab_type": "code",
        "outputId": "de040c71-f1c0-4b03-d21b-1e3ff488a601",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "s1 = tokenize('ee twisted ankle')\n",
        "s2 = tokenize('employee sprained ankle')\n",
        "w2vmodel.wv.n_similarity(s1, s2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8895315"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eZnMaQvPq6S",
        "colab_type": "text"
      },
      "source": [
        "### Compute analogies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDYQtQxePwfi",
        "colab_type": "code",
        "outputId": "26260143-d836-47ce-d86e-8ccacc9dff28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "w2vmodel.wv.most_similar_cosmul(positive=['leg', 'shoulder'], negative=['arm'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('knee', 0.9422257542610168),\n",
              " ('hip', 0.9205620288848877),\n",
              " ('ankle', 0.9082322120666504),\n",
              " ('calf', 0.8602388501167297),\n",
              " ('buttock', 0.8295111656188965),\n",
              " ('thigh', 0.8266130685806274),\n",
              " ('foot', 0.820786714553833),\n",
              " ('shin', 0.8100503087043762),\n",
              " ('heel', 0.8018286228179932),\n",
              " ('kneecap', 0.7879019975662231)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fNv9B1uS_-9",
        "colab_type": "code",
        "outputId": "6564f2f9-60da-4e47-94d9-1a52b0e063b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "w2vmodel.wv.most_similar_cosmul(positive=['rock', 'bolts'], negative=['bolt'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('slate', 0.8299375176429749),\n",
              " ('thick', 0.8163900971412659),\n",
              " ('rocks', 0.8160615563392639),\n",
              " ('shale', 0.8149546980857849),\n",
              " ('cribs', 0.8007421493530273),\n",
              " ('timbers', 0.7989218831062317),\n",
              " ('straps', 0.7926580905914307),\n",
              " ('draw', 0.7889403700828552),\n",
              " ('tons', 0.7791443467140198),\n",
              " ('cemented', 0.7789480686187744)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgB2qSVmNJLs",
        "colab_type": "text"
      },
      "source": [
        "# Weaknesses of Word2Vec\n",
        "\n",
        "One weakness of the original word2vec algorithm was that it has no way of dealing with words that were not in the original training data. Take, for example, the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "XEhWwLYLNJLt",
        "colab_type": "code",
        "outputId": "efa856b9-b673-49c5-e5a4-2f41864ccd0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "try:\n",
        "  w2vmodel.wv.most_similar('kneee')\n",
        "except KeyError as e:\n",
        "  print(e)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"word 'kneee' not in vocabulary\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTgWBDTvNJLv",
        "colab_type": "text"
      },
      "source": [
        "One solution to this is FastText, also available in Gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVK555xJNJLw",
        "colab_type": "text"
      },
      "source": [
        "# FastText with Gensim\n",
        "\n",
        "FastText is an extension of word2vec which seeks to resolve out-of-vocabulary problems by breaking words down into smaller pieces, learning embeddings for these, and then combining these pieces to produce embeddings for whole words. We accomplish this in almost exactly the same way using gensim.\n",
        "\n",
        "Parameters are as follows:\n",
        "* sentences - iterable of tokenized texts\n",
        "* size - dimensionality of learned embeddings\n",
        "* window - distance from target word considered same-context\n",
        "* min_count - the minimum number of times the word or word-piece must occur to be included in our vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ebYg1uZUNJLx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import FastText\n",
        "\n",
        "ftmodel = FastText(sentences=df_train['TOKENS'], \n",
        "                   size=100, window=15, min_count=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "0-f5tQGGNJLz",
        "colab_type": "code",
        "outputId": "a4665cf4-2c2b-4a03-9dbe-774c6d0703fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "ftmodel.wv.most_similar('knee')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('rt.knee', 0.9715948104858398),\n",
              " ('kneecap', 0.9335264563560486),\n",
              " ('kneel', 0.8777055740356445),\n",
              " ('ankle', 0.8228880763053894),\n",
              " ('knees', 0.7768344283103943),\n",
              " ('ankles', 0.7281184196472168),\n",
              " ('knew', 0.7249534130096436),\n",
              " ('elbow', 0.6834861636161804),\n",
              " ('leg', 0.6708968877792358),\n",
              " ('hip', 0.6534525156021118)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "v1J7fQYuNJL2",
        "colab_type": "code",
        "outputId": "57d6d78e-5573-4451-e258-06057f909559",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "ftmodel.wv.most_similar('kneee')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('rt.knee', 0.8781274557113647),\n",
              " ('knee', 0.8767409920692444),\n",
              " ('kneecap', 0.8357614278793335),\n",
              " ('kneel', 0.7971817255020142),\n",
              " ('knees', 0.6991230845451355),\n",
              " ('knew', 0.6636230945587158),\n",
              " ('ankle', 0.6466142535209656),\n",
              " ('ankles', 0.5612426996231079),\n",
              " ('elbow', 0.534054160118103),\n",
              " ('kneeled', 0.5328353047370911)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2H3CvWbSbW1D",
        "colab_type": "text"
      },
      "source": [
        "# Pre-Trained Embeddings\n",
        "\n",
        "Word embeddings are only as good as the data they are trained on and this regard our MSHA data has some strengths and weaknesses. On the plus side, if we're working on an MSHA related task it is reflective of the data we're working with. Lots of mining-related injury words occur in our data. On the other hand, it is a tiny slice of the text information available out there for pretraining. We can learn a lot about language from training on the rest of the data out there. One solution is to use pre-trained embeddings, that is embeddings trained on massive amounts of data, like all of wikipedia. We'll demonstrate this approach by using publicly available embeddings that have already been trained on massive datasets.\n",
        "\n",
        "For more information about the embeddings available for download see: [Gensim Data API Documentation](https://github.com/RaRe-Technologies/gensim-data)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "9Yh1RNqBNJMX",
        "colab_type": "code",
        "outputId": "14bea74e-312b-482c-8472-1b79edc14f89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# download the pretrained embeddings\n",
        "#glove_vectors = api.load(\"glove-wiki-gigaword-100\")\n",
        "#cn_vectors = api.load(\"conceptnet-numberbatch-17-06-300\")\n",
        "pre_ft_vectors = api.load('fasttext-wiki-news-subwords-300')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-7sKwsNsRjk",
        "colab_type": "code",
        "outputId": "1851f161-3956-4298-8789-b9619dede61a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "pre_ft_vectors.most_similar('knee')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('ankle', 0.8057968616485596),\n",
              " ('elbow', 0.7759488224983215),\n",
              " ('knees', 0.7516900897026062),\n",
              " ('kneecap', 0.7214105129241943),\n",
              " ('thigh', 0.7110310196876526),\n",
              " ('knee-', 0.6905907392501831),\n",
              " ('groin', 0.6795806884765625),\n",
              " ('bended', 0.6676689982414246),\n",
              " ('knee-cap', 0.6633883714675903),\n",
              " ('shoulder', 0.6611182689666748)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3J8z3EbNJMR",
        "colab_type": "text"
      },
      "source": [
        "# References\n",
        "* [Gensim Word2Vec Documentation](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec)\n",
        "* [Gensim FastText Documentation](https://radimrehurek.com/gensim/models/fasttext.html)\n",
        "* [Gensim KeyedVectors Documentation](https://radimrehurek.com/gensim/models/keyedvectors.html#module-gensim.models.keyedvectors)\n",
        "* [Gensim Data Download API](https://github.com/RaRe-Technologies/gensim-data) - describes which pretrained embeddings are available for download\n",
        "* [Word Mover Distance Paper](http://proceedings.mlr.press/v37/kusnerb15.pdf) - describes an effective measure of similarity between documents\n"
      ]
    }
  ]
}