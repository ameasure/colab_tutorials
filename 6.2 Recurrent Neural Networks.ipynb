{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "We have seen that convolutional neural networks allow us to efficiently model local structure in our data, but certainly this is not the only sort of structure that exists in nature. What else is there?\n",
    "\n",
    "Consider the process of reading a document and understanding what it means. Locality is certainly still relevant, words close to each other are certainly more likely to relate to each other. But the relationships are not limited to any fixed size. A novel might mention some characteristic of the protagonist on the first page, and not refer to that characteristic again until the very last chapter. A convolution, limited as it is to a predefined size, does not seem like a good fit for modeling potentially infinitely long relationships. Clearly its possible however, after all, humans are known to use information aquired decades in the past to inform their current decision making. What sort of model could allow us to capture relationships between inputs separated by such large distances?\n",
    "\n",
    "One option is a recurrent neural network. In its simplest form, a recurrent neural network is simply a single dense layer which is applied repeatedly to a sequence of inputs, and uses as an additional input, its output from the previous step in the sequence. In theory, such a model can capture relationships between inputs separated by an infinite distance. \n",
    "\n",
    "![Images](Images/rnn_loop.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with convolutional layers, it is often helpful to follow a recurrent layer with some sort of aggregation operation. Here, however, there is one additional option. We could just use the RNN's output from the very last step of the sequence. Since an RNN is theoretically capable of remembering all relevant information from the sequence this works. As a practical matter however, RNN's tend to forget information from distant inputs so its generally useful to use the intermediate outputs as well if those are of relevance. These can be captured through mean or max pooling (as we saw with convolutional neural networks), or through attention mechanisms which use additional layers to weight each output before averaging. \n",
    "\n",
    "### Preparing the Data\n",
    "\n",
    "We will prepare our data as we did with convolutional neural networks, by creating a sequence of vectors corresponding to the input words in our training narratives. We accomplish this with a combination of the Keras Tokenizer, which maps narrative to a sequence of numbers representing each word in the narrative, and then an Embedding layer which maps each index to a vector. This is equivalent to representing each input word with a 1-hot vector and then multiplying each of these 1-hot vectors by a Dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES set to 2\n"
     ]
    }
   ],
   "source": [
    "from cac_net.gpu_manager import set_cuda_visible_devices\n",
    "\n",
    "set_cuda_visible_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# read in our training data\n",
    "df_train = pd.read_hdf('Data/msha_2010-2011.h5')\n",
    "# read in our validation data\n",
    "df_valid = pd.read_hdf('Data/msha_2012.h5')\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df_train['NARRATIVE'])\n",
    "X_train_seq = tokenizer.texts_to_sequences(df_train['NARRATIVE'])\n",
    "X_valid_seq = tokenizer.texts_to_sequences(df_valid['NARRATIVE'])\n",
    "\n",
    "# keras only accepts a one-hot encoding of the training labels\n",
    "# we do that here\n",
    "label_encoder = LabelBinarizer().fit(df_train['INJ_BODY_PART'])\n",
    "y_train = label_encoder.transform(df_train['INJ_BODY_PART'])\n",
    "y_valid = label_encoder.transform(df_valid['INJ_BODY_PART'])\n",
    "n_codes = len(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[244, 29, 7152, 1570, 764, 213, 970, 4, 3198, 139, 5, 1924, 424, 223, 610, 1, 764, 29, 10, 1, 1570, 9, 3, 64, 2, 490, 110, 5, 213, 1, 764, 813, 4, 164, 317, 11, 6, 15, 54]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_seq[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although an RNN can, in theory, work with sequences of arbitrary length, for computational reasons it is necessary to make sure that each batch of input examples has the same length. We accomplsih this as we did with the convolutional neural networks, by padding (or truncating) all input sequences to the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0  244   29 7152 1570  764  213  970\n",
      "    4 3198  139    5 1924  424  223  610    1  764   29   10    1 1570\n",
      "    9    3   64    2  490  110    5  213    1  764  813    4  164  317\n",
      "   11    6   15   54]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "X_train_seq = sequence.pad_sequences(X_train_seq, maxlen=200)\n",
    "X_valid_seq = sequence.pad_sequences(X_valid_seq, maxlen=200)\n",
    "\n",
    "print(X_train_seq[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to specify the recurrent neural network. Here we use a particular type of recurrent neural network called an LSTM, which stands for Long-Short-Term-Memory. This contains a number of modifications which improve the performance. We will not go into the details in this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from keras.layers import Embedding, LSTM, GlobalMaxPooling1D, Concatenate\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "input_text = Input(shape=(200,), dtype='int32')\n",
    "embedding = Embedding(input_dim=len(tokenizer.word_index), \n",
    "                      output_dim=300, \n",
    "                      input_length=200)(input_text)\n",
    "lstm = LSTM(units=256, \n",
    "            dropout=0.5, \n",
    "            recurrent_dropout=0.5, \n",
    "            return_sequences=True, )(embedding)\n",
    "pool = GlobalMaxPooling1D()(lstm)\n",
    "dropout = Dropout(0.5)(pool)\n",
    "output = Dense(len(label_encoder.classes_), activation='softmax')(dropout)\n",
    "\n",
    "model = Model(inputs=input_text, outputs=output)\n",
    "optimer = Adam(lr=.001)\n",
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18681 samples, validate on 9032 samples\n",
      "Epoch 1/20\n",
      "18681/18681 [==============================] - 12s 665us/step - loss: 3.3124 - acc: 0.1376 - val_loss: 3.0416 - val_acc: 0.1784\n",
      "Epoch 2/20\n",
      "18681/18681 [==============================] - 11s 598us/step - loss: 3.0917 - acc: 0.1781 - val_loss: 2.8815 - val_acc: 0.2005\n",
      "Epoch 3/20\n",
      "18681/18681 [==============================] - 11s 595us/step - loss: 2.6774 - acc: 0.3333 - val_loss: 2.1616 - val_acc: 0.4595\n",
      "Epoch 4/20\n",
      "18681/18681 [==============================] - 11s 591us/step - loss: 2.0518 - acc: 0.5030 - val_loss: 1.6645 - val_acc: 0.5858\n",
      "Epoch 5/20\n",
      "18681/18681 [==============================] - 11s 597us/step - loss: 1.6819 - acc: 0.5888 - val_loss: 1.4028 - val_acc: 0.6607\n",
      "Epoch 6/20\n",
      "18681/18681 [==============================] - 11s 596us/step - loss: 1.4313 - acc: 0.6610 - val_loss: 1.2002 - val_acc: 0.7184\n",
      "Epoch 7/20\n",
      "18681/18681 [==============================] - 11s 601us/step - loss: 1.2420 - acc: 0.7029 - val_loss: 1.0642 - val_acc: 0.7415\n",
      "Epoch 8/20\n",
      "18681/18681 [==============================] - 11s 597us/step - loss: 1.0904 - acc: 0.7302 - val_loss: 0.9894 - val_acc: 0.7522\n",
      "Epoch 9/20\n",
      "18681/18681 [==============================] - 11s 596us/step - loss: 0.9835 - acc: 0.7500 - val_loss: 0.9303 - val_acc: 0.7633\n",
      "Epoch 10/20\n",
      "18681/18681 [==============================] - 11s 597us/step - loss: 0.9002 - acc: 0.7702 - val_loss: 0.8908 - val_acc: 0.7688\n",
      "Epoch 11/20\n",
      "18681/18681 [==============================] - 11s 592us/step - loss: 0.8221 - acc: 0.7869 - val_loss: 0.8658 - val_acc: 0.7777\n",
      "Epoch 12/20\n",
      "18681/18681 [==============================] - 11s 598us/step - loss: 0.7595 - acc: 0.7984 - val_loss: 0.8486 - val_acc: 0.7776\n",
      "Epoch 13/20\n",
      "18681/18681 [==============================] - 11s 595us/step - loss: 0.7102 - acc: 0.8149 - val_loss: 0.8330 - val_acc: 0.7797\n",
      "Epoch 14/20\n",
      "18681/18681 [==============================] - 11s 599us/step - loss: 0.6619 - acc: 0.8260 - val_loss: 0.8288 - val_acc: 0.7808\n",
      "Epoch 15/20\n",
      "18681/18681 [==============================] - 11s 595us/step - loss: 0.6241 - acc: 0.8341 - val_loss: 0.8250 - val_acc: 0.7829\n",
      "Epoch 16/20\n",
      "18681/18681 [==============================] - 11s 599us/step - loss: 0.5914 - acc: 0.8407 - val_loss: 0.8243 - val_acc: 0.7859\n",
      "Epoch 17/20\n",
      "18681/18681 [==============================] - 11s 600us/step - loss: 0.5661 - acc: 0.8459 - val_loss: 0.8266 - val_acc: 0.7851\n",
      "Epoch 18/20\n",
      "18681/18681 [==============================] - 11s 597us/step - loss: 0.5382 - acc: 0.8558 - val_loss: 0.8360 - val_acc: 0.7864\n",
      "Epoch 19/20\n",
      "18681/18681 [==============================] - 11s 591us/step - loss: 0.5091 - acc: 0.8621 - val_loss: 0.8347 - val_acc: 0.7881\n",
      "Epoch 20/20\n",
      "18681/18681 [==============================] - 11s 597us/step - loss: 0.4886 - acc: 0.8678 - val_loss: 0.8470 - val_acc: 0.7853\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f419e36b940>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train_seq, y=y_train,\n",
    "          validation_data=(X_valid_seq, y_valid),\n",
    "          batch_size=512, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A popular trick that sometimes results in some additional performance is to add another recurrent layer operating in the reverse direction of the sequence. We can implement this in Keras by wrapping our RNN with a Bidirectional layer. It simply creates two LSTM layers, one running left-to-right, the other right-to-left, and concatenates their outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional\n",
    "\n",
    "\n",
    "input_text = Input(shape=(200,), dtype='int32')\n",
    "embedding = Embedding(input_dim=len(tokenizer.word_index), \n",
    "                      output_dim=300, \n",
    "                      input_length=200)(input_text)\n",
    "lstm = Bidirectional(LSTM(units=128, \n",
    "                          dropout=0.5, \n",
    "                          recurrent_dropout=0.5, \n",
    "                          return_sequences=True),\n",
    "                     merge_mode='concat')(embedding)\n",
    "pool = GlobalMaxPooling1D()(lstm)\n",
    "dropout = Dropout(0.5)(pool)\n",
    "output = Dense(len(label_encoder.classes_), activation='softmax')(dropout)\n",
    "\n",
    "model = Model(inputs=input_text, outputs=output)\n",
    "optimer = Adam(lr=.001)\n",
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18681 samples, validate on 9032 samples\n",
      "Epoch 1/20\n",
      "18681/18681 [==============================] - 24s 1ms/step - loss: 3.3387 - acc: 0.1560 - val_loss: 3.0156 - val_acc: 0.1784\n",
      "Epoch 2/20\n",
      "18681/18681 [==============================] - 21s 1ms/step - loss: 2.8840 - acc: 0.2558 - val_loss: 2.3971 - val_acc: 0.3813\n",
      "Epoch 3/20\n",
      "18681/18681 [==============================] - 21s 1ms/step - loss: 2.1882 - acc: 0.4535 - val_loss: 1.7683 - val_acc: 0.5531\n",
      "Epoch 4/20\n",
      "18681/18681 [==============================] - 21s 1ms/step - loss: 1.7133 - acc: 0.5761 - val_loss: 1.4413 - val_acc: 0.6332\n",
      "Epoch 5/20\n",
      "18681/18681 [==============================] - 21s 1ms/step - loss: 1.4412 - acc: 0.6438 - val_loss: 1.2146 - val_acc: 0.7088\n",
      "Epoch 6/20\n",
      "18681/18681 [==============================] - 21s 1ms/step - loss: 1.2241 - acc: 0.6992 - val_loss: 1.0530 - val_acc: 0.7356\n",
      "Epoch 7/20\n",
      "18681/18681 [==============================] - 21s 1ms/step - loss: 1.0593 - acc: 0.7361 - val_loss: 0.9456 - val_acc: 0.7572\n",
      "Epoch 8/20\n",
      "18681/18681 [==============================] - 21s 1ms/step - loss: 0.9416 - acc: 0.7576 - val_loss: 0.8860 - val_acc: 0.7685\n",
      "Epoch 9/20\n",
      "18681/18681 [==============================] - 21s 1ms/step - loss: 0.8528 - acc: 0.7789 - val_loss: 0.8466 - val_acc: 0.7780\n",
      "Epoch 10/20\n",
      "18681/18681 [==============================] - 21s 1ms/step - loss: 0.7823 - acc: 0.7974 - val_loss: 0.8162 - val_acc: 0.7823\n",
      "Epoch 11/20\n",
      "18681/18681 [==============================] - 21s 1ms/step - loss: 0.7206 - acc: 0.8100 - val_loss: 0.8047 - val_acc: 0.7860\n",
      "Epoch 12/20\n",
      "18681/18681 [==============================] - 21s 1ms/step - loss: 0.6621 - acc: 0.8243 - val_loss: 0.7882 - val_acc: 0.7879\n",
      "Epoch 13/20\n",
      "18681/18681 [==============================] - 21s 1ms/step - loss: 0.6184 - acc: 0.8344 - val_loss: 0.7901 - val_acc: 0.7870\n",
      "Epoch 14/20\n",
      "18681/18681 [==============================] - 21s 1ms/step - loss: 0.5777 - acc: 0.8460 - val_loss: 0.7950 - val_acc: 0.7902\n",
      "Epoch 15/20\n",
      "18681/18681 [==============================] - 21s 1ms/step - loss: 0.5459 - acc: 0.8511 - val_loss: 0.7983 - val_acc: 0.7917\n",
      "Epoch 16/20\n",
      "18681/18681 [==============================] - 21s 1ms/step - loss: 0.5137 - acc: 0.8609 - val_loss: 0.8018 - val_acc: 0.7916\n",
      "Epoch 17/20\n",
      "18681/18681 [==============================] - 21s 1ms/step - loss: 0.4845 - acc: 0.8693 - val_loss: 0.8083 - val_acc: 0.7913\n",
      "Epoch 18/20\n",
      "18681/18681 [==============================] - 21s 1ms/step - loss: 0.4591 - acc: 0.8748 - val_loss: 0.8168 - val_acc: 0.7927\n",
      "Epoch 19/20\n",
      "18681/18681 [==============================] - 21s 1ms/step - loss: 0.4392 - acc: 0.8807 - val_loss: 0.8277 - val_acc: 0.7926\n",
      "Epoch 20/20\n",
      "18681/18681 [==============================] - 21s 1ms/step - loss: 0.4194 - acc: 0.8886 - val_loss: 0.8419 - val_acc: 0.7912\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f41966eab70>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train_seq, y=y_train,\n",
    "          validation_data=(X_valid_seq, y_valid),\n",
    "          batch_size=512, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
