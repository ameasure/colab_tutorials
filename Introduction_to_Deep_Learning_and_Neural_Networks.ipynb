{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introduction to Deep Learning and Neural Networks.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ameasure/colab_tutorials/blob/master/Introduction_to_Deep_Learning_and_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkodPOiLLdog",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUfTgilkLdoj",
        "colab_type": "text"
      },
      "source": [
        "# Deep Learning and Neural Networks\n",
        "\n",
        "Deep learning is a subfield of machine learning that seeks to solve problems by learning intermediate data representations. The primary algorithm for deep learning is the deep neural network, and if you've been following developments in Artificial Intelligence over the last decade you have probably heard a lot about them. Among its many recent accomplishments, deep learning techniques are now directly responsible the recent and very dramatic advances in image recognition, speech recognition, and language understanding.\n",
        "\n",
        "Unfortunately, as you will soon see, deep neural networks also much more difficult to build than the models we have seen so far in class. Among the many challenges:\n",
        "1) Deep neural networks often require extraordinary amounts of computation\n",
        "2) Effective training of deep neural networks often relies on a wide variety of poorly understood heuristics\n",
        "3) Deep neural networks tends to have a much larger number of hyperparameters requiring expert tuning\n",
        "\n",
        "We can only partially address the first problem in this class. Specifically, BLS now has a small number of high-end GPU's (Graphical Processing Units). A GPU is basically a very cheap super-computer, optimized for linear algebra operations. Each contains several thousand simple computers with ultra-fast memory interfaces between them so they can rapidly exchange and aggregate information. Even if you had the budget for it, you could not replicate this performance with thousands of traditional computer processing units. \n",
        "\n",
        "We will be using these GPU's for our experiments. If you wish to train deep neural networks outside of BLS, Google makes free GPU's available through [Google Colab](https://colab.research.google.com/). GPU's are also relatively affordable, a good buying guide for deep learning applications is available [here](https://blog.slavv.com/picking-a-gpu-for-deep-learning-3d4795c273b9). You are also free to try training neural networks without GPU acceleration, but be warned, unless you are building something very simple, you will be waiting a long time.\n",
        "\n",
        "One consequence of the deep neural networks' exotic hardware needs is that we will also need new software capable of using that hardware. We will use the `tensorflow` library, which is the most popular tool for training deep neural networks. We will also be using the `keras` library, which is a simplified interface on top of tensorflow. A reasonable alternative is the `PyTorch` library, but we will not cover that in this class.\n",
        "\n",
        "# What is a deep neural network?\n",
        "\n",
        "A deep neural network is just an extension of logistic regression, which itself is just an extension of linear regression. Recall the basic equation for a simple linear regression. If the inputs are represented by the variables $x_1, x_2, ..., x_i$, and the weights controlling the influence of these inputs is represented by $w_1, w_2, ..., w_i$ then the equation for linear regression is:\n",
        "\n",
        "$y=w_1x_1 + w_2x_2 + ... + w_ix_i$\n",
        "\n",
        "As we saw in logistic regression, we can modify this one step further, by passing the entire output through the logistic function $f(z)=\\frac{1}{1+e^{-z}}$. This has the effect of contraining the output to be between 0 and 1. Our equation now becomes:\n",
        "\n",
        "$y=f(w_1x_1 + w_2x_2 + ... + w_ix_i)$\n",
        "\n",
        "If we draw this in graphical form, with three inputs, it resembles the following:\n",
        "![logistic regression](https://github.com/ameasure/colab_tutorials/blob/master/Images/artificial%20neuron.png?raw=1)\n",
        "\n",
        "From this perspective, logistic regression shares some similarities with the biological neuron. Like a biological neuron, logistic regression receives inputs from a variety of sources. Like a biological neuron, the influence of these inputs seems to be controlled by the strengths of the connections (represented by weights, in logistic regression). And like a biological neuron, the logistic regression uses this information to produce an output. Hence, an alternate interpretation of logistic regression, the artificial neuron. In neural network speak, this neuron has 4 key components:\n",
        "* three inputs ($x_1, x_2, x_3$)\n",
        "* three weights ($w_1, w_2, w_3$)\n",
        "* an acitivation function ($f(w,x)$, in this case the `logistic` function)\n",
        "* an output (a.k.a. activation) ($y$)\n",
        "\n",
        "Logistic regression does not, of course, acquire any magical new powers by simply changing its name. But interesting things start to happen as we extend our metaphor. What happens when we start connecting these artificial neurons to other artificial neurons, i.e. using the outputs of some neurons and inputs to others? \n",
        "\n",
        "Although we can connect our neural networks in all sorts of patterns, for computational convenience it is common to organize our artificial neurons into layers, and design our neural network by describing how our various layers connect to each other.\n",
        "\n",
        "This might sound exotic, but in fact we have already worked with a layer of artificial neurons, we just didn't know it. A single layer of artificial neurons in which each neuron is connected to all inputs and each uses the multinomial logistic function is exactly equivalent to the multinomial logistic regression we have used so extensively in this class. We will soon see however, that by connecting these layers to other layers, we can address a wide variety of issues we struggled with before. One thing that does not change however, is the fundamental process by which we calculate the weights of our artificial neural network. We still use gradient descent, specifically the following algorithm:\n",
        "1. Randomly initialize the weights of our neurons to small values.\n",
        "2. Choose a loss function (some measure of how poorly our model is predicting the training data)\n",
        "3. Calculate the model's loss on the training data\n",
        "4. Update the weights in the direction that reduces loss (using calculus)\n",
        "5. Repeat steps 3 and 4 until we're happy with the network's performance\n",
        "\n",
        "\n",
        "# Feed Forward Neural Network\n",
        "Enough talk, lets start by building one of the simplest possible neural networks, a feed forward neural network with 1 hidden layer and 1 output layer. Visually, this resembles the following (but with more neurons in each layer).\n",
        "\n",
        "![neural_network](https://github.com/ameasure/colab_tutorials/blob/master/Images/feed_forward_network_wikipedia_300px.png?raw=1)\n",
        "\n",
        "# Note for Colab Users: \n",
        "If you're running this in Google Colab make sure you open up the **Runtime** tab, select **Change runtime type** and select **GPU** as the hardware accelerator. This will make the following computations run much faster. You will also need to execute the following command to copy the msha data to your Colab instance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3t7QWSKOrgN",
        "colab_type": "code",
        "outputId": "0e9e29dd-445c-4eba-9d8e-dcbfca845e43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "!wget --no-clobber 'https://github.com/ameasure/autocoding-class/raw/master/msha.xlsx'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File ‘msha.xlsx’ already there; not retrieving.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92YeiO4sLdok",
        "colab_type": "text"
      },
      "source": [
        "### Preparing the data\n",
        "We'll use our msha data to train our first neural network. The process of preparing the training data is mostly similar to the process we used for scikit-learn, with one key difference. Unlike scikit-learn, Keras requires the outputs (the y-values) to be in matrix format where each row corresponds to a training example, each column corresponds to a part-of-body-code, and the column containing the correct code has a value of 1 and the others have a value of 0. Scikit-learn constructed this matrix for us behind the scenes automatically. We can easily perform the same operation for Keras using scikit-learn's LabelBinarizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8sRKziKLdol",
        "colab_type": "code",
        "outputId": "b838760f-d9e3-4979-bf05-6009c4bc9ee0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# read in the data and split it into training and validation\n",
        "df = pd.read_excel('msha.xlsx')\n",
        "df['ACCIDENT_YEAR'] = df['ACCIDENT_DT'].apply(lambda x: x.year)\n",
        "df['ACCIDENT_YEAR'].value_counts()\n",
        "df_train = df[df['ACCIDENT_YEAR'].isin([2010, 2011])].copy()\n",
        "df_valid = df[df['ACCIDENT_YEAR'] == 2012].copy()\n",
        "print('training rows:', len(df_train))\n",
        "print('validation rows:', len(df_valid))\n",
        "\n",
        "# create bag of words features\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(df_train['NARRATIVE'])\n",
        "X_train = vectorizer.transform(df_train['NARRATIVE'])\n",
        "X_valid = vectorizer.transform(df_valid['NARRATIVE'])\n",
        "\n",
        "# keras only accepts a one-hot encoding of the training labels\n",
        "# we do that here\n",
        "label_encoder = LabelBinarizer().fit(df_train['INJ_BODY_PART'])\n",
        "y_train = label_encoder.transform(df_train['INJ_BODY_PART'])\n",
        "y_valid = label_encoder.transform(df_valid['INJ_BODY_PART'])\n",
        "n_codes = len(label_encoder.classes_)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training rows: 18681\n",
            "validation rows: 9032\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXwjI5IqLdon",
        "colab_type": "code",
        "outputId": "45415367-9fc1-4708-faa5-be26d248dcdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(y_train[0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(18681, 11915)\n",
            "(18681, 46)\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
            " 0 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCmFyJspLdor",
        "colab_type": "text"
      },
      "source": [
        "Define the organization of our neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaR9dIw-Ldos",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "6b958eca-e42a-4ab4-d920-7d474db82955"
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Dense, Input, Dropout\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# First, we create an input layer. This tells keras\n",
        "# what the input data will look like. Specifically, that\n",
        "# the input will contain the same number of columns\n",
        "# as the X_train matrix (i.e. the number of features)\n",
        "text_input = Input(shape=(X_train.shape[1],))\n",
        "# Next, we create the first hidden layer. \n",
        "# Dense means every neuron in this layer is connected to every input\n",
        "# units specifies the number of artificial neurons in this layer\n",
        "# activation indicates the activation function that will be used on each neuron\n",
        "# (text_input) indicates that the layer uses output of the text_input layer as its input\n",
        "layer1 = Dense(units=100, activation='relu')(text_input)\n",
        "# Now we specify the ouput layer\n",
        "# it contains one neuron for each part of body code (we want predctions for these)\n",
        "# each neuron uses the softmax activation so that the outputs mimic probabilities\n",
        "# Note: this is identical to a multinomial logistic regression model\n",
        "output = Dense(units=n_codes, activation='softmax')(layer1)\n",
        "\n",
        "# Finally, we tell Keras which layers are the inputs and outputs of our model\n",
        "model = Model(inputs=[text_input], outputs=[output])\n",
        "\n",
        "# We then tell Keras how we plan to fit the model\n",
        "# optimizer - specifies the algorithm we will use to update the model weights\n",
        "#   specifically, we use a variant of gradient descent called Adam\n",
        "#   with a learning rate of .001\n",
        "# loss - specifices the loss function we will use when updating weights\n",
        "# metrics - specifies the validation metrics we will calculate after each epoch\n",
        "optimizer = Adam(lr=.001)\n",
        "model.compile(optimizer=optimizer, \n",
        "              loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0621 22:37:04.168827 140066223662976 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0621 22:37:04.184885 140066223662976 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0621 22:37:04.188314 140066223662976 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0621 22:37:04.230037 140066223662976 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0621 22:37:04.240237 140066223662976 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwkmZeysLdov",
        "colab_type": "text"
      },
      "source": [
        "Examine the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DCFMS74Ldow",
        "colab_type": "code",
        "outputId": "9f8d7fe9-9d36-45fb-a6e4-5cd6b8698461",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 11915)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100)               1191600   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 46)                4646      \n",
            "=================================================================\n",
            "Total params: 1,196,246\n",
            "Trainable params: 1,196,246\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uUiA8o4Ldo2",
        "colab_type": "text"
      },
      "source": [
        "Now that we have specified our model, we can fit it to our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EtKB0j8Ldo3",
        "colab_type": "code",
        "outputId": "0835935c-3de5-4990-83ef-9186a249a98a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        }
      },
      "source": [
        "# x - the training input to our model\n",
        "# y - the training ouput for our model\n",
        "# batch_size - the number of training examples we use for each gradient update\n",
        "# epochs - the number of full cycles through the training data\n",
        "# validation_data - the data we evaluate the model on at the end of each epoch\n",
        "model.fit(x=X_train, y=y_train,\n",
        "          validation_data=(X_valid, y_valid),\n",
        "          batch_size=32, epochs=5)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0621 22:37:04.361691 140066223662976 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0621 22:37:04.410722 140066223662976 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 18681 samples, validate on 9032 samples\n",
            "Epoch 1/5\n",
            "18681/18681 [==============================] - 5s 269us/step - loss: 1.6753 - acc: 0.6260 - val_loss: 1.0232 - val_acc: 0.7537\n",
            "Epoch 2/5\n",
            "18681/18681 [==============================] - 4s 202us/step - loss: 0.7193 - acc: 0.8228 - val_loss: 0.8959 - val_acc: 0.7637\n",
            "Epoch 3/5\n",
            "18681/18681 [==============================] - 4s 201us/step - loss: 0.4437 - acc: 0.8883 - val_loss: 0.9164 - val_acc: 0.7576\n",
            "Epoch 4/5\n",
            "18681/18681 [==============================] - 4s 201us/step - loss: 0.2959 - acc: 0.9292 - val_loss: 0.9486 - val_acc: 0.7541\n",
            "Epoch 5/5\n",
            "18681/18681 [==============================] - 4s 203us/step - loss: 0.2060 - acc: 0.9523 - val_loss: 1.0065 - val_acc: 0.7477\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f63863dadd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjRDv4XJLdo9",
        "colab_type": "text"
      },
      "source": [
        "What's going on? It's just gradient descent, the same algorithm we used to calculate the parameters of our models before. The main difference is that Keras is now showing us lots of intermediate outputs. In particular, you can see how the loss of the model changes as we grab a new batch of training examples, calculate the loss, and then update the weights. When we get to the end of each epoch you see Keras calculate the accuracy of the model on the validation data set. We do this so we know when to stop training our model, i.e. when the validation performance starts going down it's time to stop because our model is overfitting to the training data. We can plot the pattern below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99rjV928Ldo-",
        "colab_type": "code",
        "outputId": "34be9a48-7967-41a3-eb0d-797952be359f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('validation accuracy')\n",
        "plt.plot(model.history.history['val_acc'])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f63517e1748>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFRJjjDEFYoXEGGNMgfwvCq2bySfBix4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AzPxtC4LdpC",
        "colab_type": "text"
      },
      "source": [
        "This is a popular technique for determining how long to train neural networks because neural networks tend to be heavily over-parameterized, if you train them for long enough they can pften overfit the training data very easily.\n",
        "\n",
        "# Using the Model\n",
        "As with scikit-learn models, we can generate the predictions and save them to a dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKiqQAz8LdpD",
        "colab_type": "code",
        "outputId": "9ea7c744-403b-4614-f870-7a2bbe42d954",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        }
      },
      "source": [
        "# get the [n_cases, n_classes] predicted probability matrix\n",
        "prediction_prob_matrix = model.predict(X_valid)\n",
        "# get the classes with the highest predicted probability, save them to our dataframe\n",
        "df_valid['PREDICTED_PART'] = label_encoder.inverse_transform(prediction_prob_matrix)\n",
        "# add the predicted probabilities\n",
        "df_valid['PREDICTED_PROB'] = prediction_prob_matrix.max(axis=1)\n",
        "# take a look at what we've got\n",
        "df_valid.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ACCIDENT_DT</th>\n",
              "      <th>FIPS_STATE_CD</th>\n",
              "      <th>INJ_BODY_PART</th>\n",
              "      <th>INJ_BODY_PART_CD</th>\n",
              "      <th>MINE_ID</th>\n",
              "      <th>NARRATIVE</th>\n",
              "      <th>ACCIDENT_YEAR</th>\n",
              "      <th>PREDICTED_PART</th>\n",
              "      <th>PREDICTED_PROB</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2012-08-20</td>\n",
              "      <td>24</td>\n",
              "      <td>HIPS (PELVIS/ORGANS/KIDNEYS/BUTTOCKS)</td>\n",
              "      <td>440</td>\n",
              "      <td>1800761</td>\n",
              "      <td>Employee, parked s/c on grade at 16-Block #3 E...</td>\n",
              "      <td>2012</td>\n",
              "      <td>TRUNK, MULTIPLE PARTS</td>\n",
              "      <td>0.275639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2012-02-21</td>\n",
              "      <td>42</td>\n",
              "      <td>BODY SYSTEMS</td>\n",
              "      <td>600</td>\n",
              "      <td>3600017</td>\n",
              "      <td>Possible heart attack.</td>\n",
              "      <td>2012</td>\n",
              "      <td>BODY SYSTEMS</td>\n",
              "      <td>0.570566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2012-10-03</td>\n",
              "      <td>41</td>\n",
              "      <td>SHOULDERS (COLLARBONE/CLAVICLE/SCAPULA)</td>\n",
              "      <td>450</td>\n",
              "      <td>3503757</td>\n",
              "      <td>Employee was cleaning up plant spillage into a...</td>\n",
              "      <td>2012</td>\n",
              "      <td>SHOULDERS (COLLARBONE/CLAVICLE/SCAPULA)</td>\n",
              "      <td>0.802509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2012-01-23</td>\n",
              "      <td>17</td>\n",
              "      <td>FINGER(S)/THUMB</td>\n",
              "      <td>340</td>\n",
              "      <td>1103189</td>\n",
              "      <td>Employee was putting a drag on a shuttle car, ...</td>\n",
              "      <td>2012</td>\n",
              "      <td>FINGER(S)/THUMB</td>\n",
              "      <td>0.999971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>2012-05-10</td>\n",
              "      <td>42</td>\n",
              "      <td>HAND (NOT WRIST OR FINGERS)</td>\n",
              "      <td>330</td>\n",
              "      <td>4600016</td>\n",
              "      <td>While using a cutting torch to remove a bearin...</td>\n",
              "      <td>2012</td>\n",
              "      <td>HAND (NOT WRIST OR FINGERS)</td>\n",
              "      <td>0.880725</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ACCIDENT_DT  ...  PREDICTED_PROB\n",
              "2   2012-08-20  ...        0.275639\n",
              "5   2012-02-21  ...        0.570566\n",
              "8   2012-10-03  ...        0.802509\n",
              "12  2012-01-23  ...        0.999971\n",
              "27  2012-05-10  ...        0.880725\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcI2qU3ILdpG",
        "colab_type": "text"
      },
      "source": [
        "# Regularization\n",
        "\n",
        "Even more so than linear models, neural networks suffer from overfitting. Because they have such a large number of parameters they can easily memorize the data they are trained on. We can use L2-regularization, as we did with our logistic regression models, but for neural networks another popular technique is called `dropout`. In dropout we randomly set the outputs of a fraction of neurons, at each training step, to 0. A popular choice is 50%. This has a strong regularization effect that often improves overall performance. Let's try it out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1LMKxLcLdpH",
        "colab_type": "code",
        "outputId": "bc74b8a3-0ca0-46be-c737-8b55643c36cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "text_input = Input(shape=(X_train.shape[1],))\n",
        "layer1 = Dense(units=100, activation='relu')(text_input)\n",
        "dropout = Dropout(0.5)(layer1)\n",
        "output = Dense(units=len(label_encoder.classes_), activation='softmax')(dropout)\n",
        "# specify the inputs and outputs of our model\n",
        "# input is the raw text features\n",
        "# output is the predicted probabilities\n",
        "do_model = Model(inputs=[text_input], outputs=[output])\n",
        "# specify the algorithm for calculating weights 'adam'\n",
        "# specify the loss function 'categorical_crossentropy'\n",
        "# specify the validation metrics we will calculate after each epoch\n",
        "optimizer = Adam(lr=.001)\n",
        "do_model.compile(optimizer=optimizer, \n",
        "                loss='categorical_crossentropy', \n",
        "                metrics=['accuracy'])\n",
        "\n",
        "do_model.fit(x=X_train, y=y_train,\n",
        "            validation_data=(X_valid, y_valid),\n",
        "            batch_size=32, epochs=5)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0621 22:37:25.669502 140066223662976 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 18681 samples, validate on 9032 samples\n",
            "Epoch 1/5\n",
            "18681/18681 [==============================] - 4s 229us/step - loss: 1.9940 - acc: 0.5392 - val_loss: 1.1713 - val_acc: 0.7259\n",
            "Epoch 2/5\n",
            "18681/18681 [==============================] - 4s 216us/step - loss: 1.0529 - acc: 0.7472 - val_loss: 0.9290 - val_acc: 0.7614\n",
            "Epoch 3/5\n",
            "18681/18681 [==============================] - 4s 208us/step - loss: 0.7707 - acc: 0.8042 - val_loss: 0.8614 - val_acc: 0.7729\n",
            "Epoch 4/5\n",
            "18681/18681 [==============================] - 4s 207us/step - loss: 0.6099 - acc: 0.8405 - val_loss: 0.8418 - val_acc: 0.7715\n",
            "Epoch 5/5\n",
            "18681/18681 [==============================] - 4s 209us/step - loss: 0.5076 - acc: 0.8669 - val_loss: 0.8440 - val_acc: 0.7723\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6350e30080>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEdr0St1LdpK",
        "colab_type": "code",
        "outputId": "7ba02997-e480-4bdb-8cd1-5aaea34d816f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "plt.ylabel('validation accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.plot(do_model.history.history['val_acc'], label='with dropout')\n",
        "plt.plot(model.history.history['val_acc'], label='without dropout')\n",
        "plt.legend()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f6350a58518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSGHl6KQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDu8JeZrLdpQ",
        "colab_type": "text"
      },
      "source": [
        "As you can see, adding dropout improved the overall performance of our model and decreased overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4mQC6oynGaT",
        "colab_type": "text"
      },
      "source": [
        "# Uncertainty Estimates and \"Bayesian Neural Networks\"\n",
        "Our model already us estimates of uncertainty about classifications in the form of probability values, but can it also give us estimates of uncertainty in its probability predictions? [Yarin Gal and collaborators](https://arxiv.org/pdf/1506.02142.pdf) show that it can because training a neural network with dropout is equivalent to approximate Bayesian inference in a deep Gaussian process. To estimate the uncertainty of these predictions all we need to do is sample from the distribution by leaving dropout on while generating predictions. We illustrate this below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN1HZSEbndDI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "bd42b6c0-ad62-4504-d549-8ef4d4823057"
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# create a dropout layer that stays on even during prediction\n",
        "class MCDropout(keras.layers.Dropout):\n",
        "  def call(self, inputs):\n",
        "    return super().call(inputs, training=True)\n",
        " \n",
        "# create a dropout model that keeps dropout on\n",
        "mc_text_input = Input(shape=(X_train.shape[1],))\n",
        "mc_layer1 = Dense(units=100, activation='relu')(mc_text_input)\n",
        "mc_dropout = MCDropout(0.5)(mc_layer1)\n",
        "mc_output = Dense(units=len(label_encoder.classes_), activation='softmax')(mc_dropout)\n",
        "mc_model = Model(inputs=[mc_text_input], outputs=[mc_output])\n",
        "optimizer = Adam(lr=.001)\n",
        "mc_model.compile(optimizer=optimizer, \n",
        "                loss='categorical_crossentropy', \n",
        "                metrics=['accuracy'])\n",
        "\n",
        "mc_model.fit(x=X_train, y=y_train,\n",
        "             validation_data=(X_valid, y_valid),\n",
        "             batch_size=32, epochs=5)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 18681 samples, validate on 9032 samples\n",
            "Epoch 1/5\n",
            "18681/18681 [==============================] - 4s 219us/step - loss: 2.0169 - acc: 0.5343 - val_loss: 1.3106 - val_acc: 0.6984\n",
            "Epoch 2/5\n",
            "18681/18681 [==============================] - 4s 200us/step - loss: 1.0606 - acc: 0.7470 - val_loss: 1.0737 - val_acc: 0.7347\n",
            "Epoch 3/5\n",
            "18681/18681 [==============================] - 4s 206us/step - loss: 0.7801 - acc: 0.8027 - val_loss: 1.0013 - val_acc: 0.7427\n",
            "Epoch 4/5\n",
            "18681/18681 [==============================] - 4s 205us/step - loss: 0.6174 - acc: 0.8405 - val_loss: 1.0040 - val_acc: 0.7430\n",
            "Epoch 5/5\n",
            "18681/18681 [==============================] - 4s 205us/step - loss: 0.5074 - acc: 0.8668 - val_loss: 1.0171 - val_acc: 0.7437\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f63509e7c88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wSuD92LIrVJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sample predictions from our model\n",
        "y_probas = np.stack([mc_model.predict(X_valid) for sample in range(20)])\n",
        "# recover the \"best predictions\" by averaging over the predicted probabilities\n",
        "y_prob = y_probas.mean(axis=0)\n",
        "# we can also recover the standard deviations\n",
        "y_std = y_probas.std(axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86bR6Ubg-oWa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the standard deviation just for the predicted code\n",
        "y_pred_std = y_std[range(len(y_prob)), y_prob.argmax(axis=1)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQfZrXKU0-Jz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "outputId": "17154366-3e38-4c53-b98d-8c721d596149"
      },
      "source": [
        "df_valid['BAYES_PRED'] = label_encoder.inverse_transform(y_prob)\n",
        "df_valid['BAYES_PROB'] = y_prob.max(axis=1)\n",
        "df_valid['BAYES_STD'] = np.round(y_pred_std, 2)\n",
        "df_valid[['NARRATIVE', 'PREDICTED_PART', 'PREDICTED_PROB', 'BAYES_PRED', 'BAYES_PROB', 'BAYES_STD']].sample(10)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>NARRATIVE</th>\n",
              "      <th>PREDICTED_PART</th>\n",
              "      <th>PREDICTED_PROB</th>\n",
              "      <th>BAYES_PRED</th>\n",
              "      <th>BAYES_PROB</th>\n",
              "      <th>BAYES_STD</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>19670</th>\n",
              "      <td>EE was conducting routine daily walk-around in...</td>\n",
              "      <td>BODY SYSTEMS</td>\n",
              "      <td>0.982711</td>\n",
              "      <td>BODY SYSTEMS</td>\n",
              "      <td>0.740377</td>\n",
              "      <td>0.24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2379</th>\n",
              "      <td>Employee stated that on 3/6/12 a rubber Main F...</td>\n",
              "      <td>LOWER LEG/TIBIA/FIBULA</td>\n",
              "      <td>0.952950</td>\n",
              "      <td>LOWER LEG/TIBIA/FIBULA</td>\n",
              "      <td>0.839868</td>\n",
              "      <td>0.12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25200</th>\n",
              "      <td>Employee was using a belt knife to cut a zip t...</td>\n",
              "      <td>FINGER(S)/THUMB</td>\n",
              "      <td>0.989689</td>\n",
              "      <td>FINGER(S)/THUMB</td>\n",
              "      <td>0.917245</td>\n",
              "      <td>0.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5517</th>\n",
              "      <td>Employee was opening a can of welding rod and ...</td>\n",
              "      <td>FINGER(S)/THUMB</td>\n",
              "      <td>0.997874</td>\n",
              "      <td>FINGER(S)/THUMB</td>\n",
              "      <td>0.976530</td>\n",
              "      <td>0.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3201</th>\n",
              "      <td>Employee was helping to install a guard on a b...</td>\n",
              "      <td>FINGER(S)/THUMB</td>\n",
              "      <td>0.994660</td>\n",
              "      <td>FINGER(S)/THUMB</td>\n",
              "      <td>0.991058</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>129</th>\n",
              "      <td>A grating was leaned against a closed overhead...</td>\n",
              "      <td>KNEE/PATELLA</td>\n",
              "      <td>0.841419</td>\n",
              "      <td>KNEE/PATELLA</td>\n",
              "      <td>0.852884</td>\n",
              "      <td>0.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13794</th>\n",
              "      <td>Was moving an existing double door to new side...</td>\n",
              "      <td>MULTIPLE PARTS (MORE THAN ONE MAJOR)</td>\n",
              "      <td>0.971716</td>\n",
              "      <td>MULTIPLE PARTS (MORE THAN ONE MAJOR)</td>\n",
              "      <td>0.700414</td>\n",
              "      <td>0.24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31042</th>\n",
              "      <td>Operator was panning down haul road in a scrap...</td>\n",
              "      <td>MULTIPLE PARTS (MORE THAN ONE MAJOR)</td>\n",
              "      <td>0.743675</td>\n",
              "      <td>MULTIPLE PARTS (MORE THAN ONE MAJOR)</td>\n",
              "      <td>0.525752</td>\n",
              "      <td>0.28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9811</th>\n",
              "      <td>As employee was installing a roof bolt he felt...</td>\n",
              "      <td>SHOULDERS (COLLARBONE/CLAVICLE/SCAPULA)</td>\n",
              "      <td>0.999773</td>\n",
              "      <td>SHOULDERS (COLLARBONE/CLAVICLE/SCAPULA)</td>\n",
              "      <td>0.984976</td>\n",
              "      <td>0.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20319</th>\n",
              "      <td>Employee was shoveling belt when he felt pain ...</td>\n",
              "      <td>BACK (MUSCLES/SPINE/S-CORD/TAILBONE)</td>\n",
              "      <td>0.948398</td>\n",
              "      <td>BACK (MUSCLES/SPINE/S-CORD/TAILBONE)</td>\n",
              "      <td>0.946493</td>\n",
              "      <td>0.04</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               NARRATIVE  ... BAYES_STD\n",
              "19670  EE was conducting routine daily walk-around in...  ...      0.24\n",
              "2379   Employee stated that on 3/6/12 a rubber Main F...  ...      0.12\n",
              "25200  Employee was using a belt knife to cut a zip t...  ...      0.08\n",
              "5517   Employee was opening a can of welding rod and ...  ...      0.02\n",
              "3201   Employee was helping to install a guard on a b...  ...      0.01\n",
              "129    A grating was leaned against a closed overhead...  ...      0.10\n",
              "13794  Was moving an existing double door to new side...  ...      0.24\n",
              "31042  Operator was panning down haul road in a scrap...  ...      0.28\n",
              "9811   As employee was installing a roof bolt he felt...  ...      0.02\n",
              "20319  Employee was shoveling belt when he felt pain ...  ...      0.04\n",
              "\n",
              "[10 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bM08fQ6p6LFb",
        "colab_type": "text"
      },
      "source": [
        "As an added bonus this process often results in a slight improvement in model performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrzFaYzv2Och",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "0dff010b-755a-4fcd-8a33-4c7dd5b712db"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "normal_acc = accuracy_score(y_true=df_valid['INJ_BODY_PART'], \n",
        "                            y_pred=df_valid['PREDICTED_PART'])\n",
        "bayes_acc = accuracy_score(y_true=df_valid['INJ_BODY_PART'], \n",
        "                           y_pred=df_valid['BAYES_PRED'])\n",
        "# average macro f1 over the same set of labels for fair comparison\n",
        "labels = df_valid['INJ_BODY_PART'].unique()\n",
        "normal_mf1 = f1_score(y_true=df_valid['INJ_BODY_PART'], y_pred=df_valid['PREDICTED_PART'], \n",
        "                      labels=labels, average='macro')\n",
        "bayes_mf1 = f1_score(y_true=df_valid['INJ_BODY_PART'], y_pred=df_valid['BAYES_PRED'], \n",
        "                     labels=labels, average='macro')\n",
        "\n",
        "print(f'orig acc: {normal_acc:.3f}')\n",
        "print(f'bayes acc: {bayes_acc:.3f}')\n",
        "print(f'orig macro f1: {normal_mf1:.3f}')\n",
        "print(f'bayes macro f1: {bayes_mf1:.3f}')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "orig acc: 0.748\n",
            "bayes acc: 0.770\n",
            "orig macro f1: 0.541\n",
            "bayes macro f1: 0.542\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "PGpqI_-_LdpQ",
        "colab_type": "text"
      },
      "source": [
        "# When and why do deep neural networks work?\n",
        "It's still a mystery, but hints are starting to emerge. For example, [[1]](https://arxiv.org/pdf/1608.08225.pdf) points out that hierarchical and compositional relationships are very common in nature and deep neural networks can represent these relationships with exponentially fewer parameters than shallow networks (linear models). It can also be shown that neural networks are universal function approximators, in other words they can approximate any relationship between inputs and outputs to any degree of precision. \n",
        "\n",
        "In general, deep neural networks seem to work best on tasks where the input contains large amounts of extraneous noise, such as image and speech understanding (not math or logic). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FerKuE__12Q",
        "colab_type": "text"
      },
      "source": [
        "# Next Lesson\n",
        "\n",
        "[Convolutional Neural Networks](https://colab.research.google.com/drive/1Bck3x0znv61uDSkZIT5pAdO_oyWaUPup)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uD_IolOG_uem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}