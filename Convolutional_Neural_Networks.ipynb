{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Convolutional Neural Networks.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ameasure/colab_tutorials/blob/master/Convolutional_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2bKwlhxB-DC",
        "colab_type": "code",
        "outputId": "ac258c62-635b-4113-90fe-c0d5833bcacd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "!wget 'https://github.com/ameasure/autocoding-class/raw/master/msha.xlsx'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-06-17 21:26:10--  https://github.com/ameasure/autocoding-class/raw/master/msha.xlsx\n",
            "Resolving github.com (github.com)... 13.229.188.59\n",
            "Connecting to github.com (github.com)|13.229.188.59|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/ameasure/autocoding-class/master/msha.xlsx [following]\n",
            "--2019-06-17 21:26:11--  https://raw.githubusercontent.com/ameasure/autocoding-class/master/msha.xlsx\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4183086 (4.0M) [application/octet-stream]\n",
            "Saving to: ‘msha.xlsx.1’\n",
            "\n",
            "msha.xlsx.1         100%[===================>]   3.99M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2019-06-17 21:26:11 (80.5 MB/s) - ‘msha.xlsx.1’ saved [4183086/4183086]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZW8pscPBg-o",
        "colab_type": "text"
      },
      "source": [
        "# Convolutional Neural Networks\n",
        "\n",
        "In our previous class we introduced the densely connected neural network with a single hidden layer. I pointed out that there has been some theoretical work showing such a model can approximate any relationship between the inputs and outputs to an arbitrary degree. None the less, when we tried it out on MSHA coding we saw no significant improvements. Why?\n",
        "\n",
        "The problem is that it is not enough for a model to be theoretically capable of representing any relationship, it must also be able to learn that relationship from the training data that is available. Here, there are no guarantees. The more flexible our model is, the more possibilities it has to rule out based on the training data. Conversely, the more correctly we can specify the model up front, the less burden we place on the training data. This is sometimes called an \"inductive bias.\" One of the big advantages of deep neural networks is that they give us a lot of flexibility in specifying inductive biases.  \n",
        "\n",
        "### Locality\n",
        "So what sorts of biases might exist in our data? Obviously it depends on the task, but there are some biases that seem to be very common in nature. One of these is locality. Locality simply means that things that are close to each other are much more likely to interact with each other. This occurs at all sorts of levels in all sorts of ways. If you're an antelope on the savannah, a lion right next to you is a serious threat, a lion 10,000 miles away is completely irrelevant. If you're a particle hurtling through space, the fact that there's a planet a million miles away is much less relevant to your flight path then the fact that there's another particle just in front of you that you're about to collide with. If you're a pixel on an image, the pixel right next to you is much more likely to be similar to you than the one on the other side of the image. If you're a word in a document, your meaning is much more likely to be affected by other words in the same sentence than by words in some distant paragraph. Locality is basically everywhere in the natural world yet our densely connected neural network has no such bias, it simply allows every input to interact with every other input and all of these are treated initially as equally likely possibilities. So an important question is, can we constrain our model to better reflect the bias toward locality? The answer is yes. One way to do this is with a convolutional layer.\n",
        "\n",
        "### Convolutional Layer\n",
        "In its simplest form, a convolutional layer is simply one or more densely connected artificial neurons that are applied to multiple subsets of the input. Typically these subsets are contiguous (locally connected), reflecting a locality bias. By constraining our neurons in this way we force them to consider only local interactions that exist in these subsets. Information about these interactions can then be passed to later layers (potentially also convolutional) to form a larger understanding of the overall input. \n",
        "\n",
        "This idea has enjoyed enormous success in computer vision and is now used extensively, but imagery is not the only input that exhibits locality. Language also has this. For example, words that occur next to each other in a sentence are far more likely to be related to each other. The same is true for letters. Some of the same insights from vision also apply to language.\n",
        "\n",
        "### Word Convolutions\n",
        "Consider, for example, the sentence \"the man fell on his left side.\" If we were to represent each of the words in this sentence by a vector, we could represent the entire input by the concatenation of these vectors. We could then perform a convolutional operation across each 3 word sub-sequence in this sentence as follows:\n",
        "\n",
        "![Images](https://github.com/ameasure/colab_tutorials/blob/master/Images/1d_conv_nopool_loop.gif?raw=1)\n",
        "\n",
        "Note, in this particular convolutional layer we have two artificial neurons. Neurons in a convolutional layer are sometimes called `filters` because you can think of them as \"filtering\" the input for particular patterns. Like any articial neuron each filter has weights, controlling how each input contributes to the final output, and a bias. As each filter is applied to each continuous 3-word sequence in our sentence, it generates an output. The final result is that for just this one sentence, each filter has generated 6 outputs corresponding to the 6 continuous 3-word sequences found in our sentence. Because there tends to be lots of redundant information in the output of each filter, it is common to aggregate this information. One approach is `max pooling`, i.e. simply  take the highest value produced by each filter. The result is a single vector of output containing 2 values, corresponding to the highest values produced by each of our filters. The entire computation is illustrated below:\n",
        "\n",
        "![Images](https://github.com/ameasure/colab_tutorials/blob/master/Images/1d_conv_withpool_loop.gif?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-m22ZczGBg-6",
        "colab_type": "text"
      },
      "source": [
        "We implement this model below, in Keras. \n",
        "\n",
        "### Preparing the Data\n",
        "\n",
        "The first step is to prepare the input data and here we diverge from our previous approach. In the past we used the bag-of-words approach, discarding all information about the order in which words appear. Now that we're working with convolutions, we need to preserve this information. We will accomplish this by using the Keras Tokenizer to map each word to a unique number, and then representing the sequence of words in each our narratives by the corresponding sequence of numbers. Although this ends up happening behind the scenes, this is equivalent to representing each word with a one-hot-encoding and stacking the one-hot-encodings sequentially."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4msdvfvBg-8",
        "colab_type": "code",
        "outputId": "71bba6c3-935d-47c0-e7d0-7d8009321ab0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "df = pd.read_excel('msha.xlsx')\n",
        "df['ACCIDENT_YEAR'] = df['ACCIDENT_DT'].apply(lambda x: x.year)\n",
        "df['ACCIDENT_YEAR'].value_counts()\n",
        "df_train = df[df['ACCIDENT_YEAR'].isin([2010, 2011])].copy()\n",
        "df_valid = df[df['ACCIDENT_YEAR'] == 2012].copy()\n",
        "print('training rows:', len(df_train))\n",
        "print('validation rows:', len(df_valid))\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df_train['NARRATIVE'])\n",
        "X_train_seq = tokenizer.texts_to_sequences(df_train['NARRATIVE'])\n",
        "X_valid_seq = tokenizer.texts_to_sequences(df_valid['NARRATIVE'])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training rows: 18681\n",
            "validation rows: 9032\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onRKmYKSBg_E",
        "colab_type": "code",
        "outputId": "c042886b-248c-40ca-eebf-eb18554a153a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(X_train_seq[0])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[244, 29, 7152, 1570, 764, 213, 970, 4, 3198, 139, 5, 1924, 424, 223, 610, 1, 764, 29, 10, 1, 1570, 9, 3, 64, 2, 490, 110, 5, 213, 1, 764, 813, 4, 164, 317, 11, 6, 15, 54]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzSjdinDBg_L",
        "colab_type": "text"
      },
      "source": [
        "As you can see, the Keras tokenizer has converted our narrative into a list of numbers, each corresponding to a word. There is, however, one more modification we need to make. Because each narrative contains a different number of words, but all our neural network layers contain a fixed number of weights, we need to figure out what to do with the mismatch. The simplest approach is simply to pad each narrative to the same length with special \"blank\" words (representer by the number 0). We accomplish this using the pad_sequences function from Keras, padding each narrative to 200 words (or truncating it to 200 words, if it is longer). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfsl1d9tBg_N",
        "colab_type": "code",
        "outputId": "c6d03d3c-10ff-45dc-b8b2-06554d3ee5e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "from keras.preprocessing import sequence\n",
        "\n",
        "X_train_seq = sequence.pad_sequences(X_train_seq, maxlen=200)\n",
        "X_valid_seq = sequence.pad_sequences(X_valid_seq, maxlen=200)\n",
        "\n",
        "print(X_train_seq[0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0  244   29 7152 1570  764  213  970\n",
            "    4 3198  139    5 1924  424  223  610    1  764   29   10    1 1570\n",
            "    9    3   64    2  490  110    5  213    1  764  813    4  164  317\n",
            "   11    6   15   54]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkTTaWQdBg_U",
        "colab_type": "text"
      },
      "source": [
        "Our training inputs are now ready, we just need to prepare the training outputs. Keras requires these to be in a one-hot encoding. We do that below using sklearn's LabelBinarizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJDa7OkwBg_W",
        "colab_type": "code",
        "outputId": "25355283-e57d-421d-e2e5-ad0911239153",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "# keras only accepts a one-hot encoding of the training labels\n",
        "# we do that here\n",
        "label_encoder = LabelBinarizer().fit(df_train['INJ_BODY_PART'])\n",
        "y_train = label_encoder.transform(df_train['INJ_BODY_PART'])\n",
        "y_valid = label_encoder.transform(df_valid['INJ_BODY_PART'])\n",
        "n_codes = len(label_encoder.classes_)\n",
        "print(y_train[0])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
            " 0 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQS3OpllBg_d",
        "colab_type": "text"
      },
      "source": [
        "We're now ready to specify the convolutional model. Here we use a single convolutional layer with 100 filters, each operating over 3-word subsets of the input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I04l3yN0Bg_f",
        "colab_type": "code",
        "outputId": "cea3dddb-6c46-4746-f74a-544703bce669",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Dense, Input, Dropout\n",
        "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Concatenate\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "input_text = Input(shape=(200,), dtype='int32')\n",
        "embedding = Embedding(len(tokenizer.word_index), \n",
        "                          300, \n",
        "                          input_length=200)(input_text)\n",
        "dropout = Dropout(0.1)(embedding)\n",
        "convolution = Conv1D(filters=100, \n",
        "                     kernel_size=3,\n",
        "                     padding='valid',\n",
        "                     strides=1,\n",
        "                     activation='relu')(dropout)\n",
        "pool = GlobalMaxPooling1D()(convolution)\n",
        "dense = Dense(100, activation='relu')(pool)\n",
        "dropout = Dropout(0.5)(dense)\n",
        "output = Dense(len(label_encoder.classes_), activation='softmax')(dense)\n",
        "\n",
        "conv_model = Model(inputs=input_text, outputs=output)\n",
        "\n",
        "conv_model.compile(optimizer='adam', \n",
        "                  loss='categorical_crossentropy', \n",
        "                  metrics=['accuracy'])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0617 21:26:19.162114 139626775693184 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0617 21:26:19.183620 139626775693184 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0617 21:26:19.187818 139626775693184 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0617 21:26:19.201766 139626775693184 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0617 21:26:19.211415 139626775693184 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0617 21:26:19.288308 139626775693184 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0617 21:26:19.312315 139626775693184 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "II_9oecSBg_j",
        "colab_type": "code",
        "outputId": "8bb39793-9755-4412-a206-f58f5d1fb5c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "conv_model.fit(x=X_train_seq, y=y_train,\n",
        "               validation_data=(X_valid_seq, y_valid),\n",
        "               batch_size=32, epochs=5)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0617 21:26:19.586082 139626775693184 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 18681 samples, validate on 9032 samples\n",
            "Epoch 1/5\n",
            "18681/18681 [==============================] - 9s 458us/step - loss: 1.5289 - acc: 0.6213 - val_loss: 0.8607 - val_acc: 0.7766\n",
            "Epoch 2/5\n",
            "18681/18681 [==============================] - 5s 269us/step - loss: 0.7771 - acc: 0.7896 - val_loss: 0.7928 - val_acc: 0.7880\n",
            "Epoch 3/5\n",
            "18681/18681 [==============================] - 5s 270us/step - loss: 0.5611 - acc: 0.8432 - val_loss: 0.8158 - val_acc: 0.7827\n",
            "Epoch 4/5\n",
            "18681/18681 [==============================] - 5s 269us/step - loss: 0.3835 - acc: 0.8909 - val_loss: 0.8652 - val_acc: 0.7822\n",
            "Epoch 5/5\n",
            "18681/18681 [==============================] - 5s 272us/step - loss: 0.2371 - acc: 0.9346 - val_loss: 0.9600 - val_acc: 0.7751\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7efd19554b38>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qv95Dy1Bg_s",
        "colab_type": "text"
      },
      "source": [
        "There's nothing magical about convolving over 3-word subsequences, an alternate is to have multiple convolutional layers each operating over different length subsequences. Here, we create convolutional layers for 2, 3, 4, and 5 word subsequences. The resulting outputs are then concatenated before being fed to subsequent layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7z6xF06Bg_u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_text = Input(shape=(200,), dtype='int32')\n",
        "embedding = Embedding(len(tokenizer.word_index), \n",
        "                          300, \n",
        "                          input_length=200)(input_text)\n",
        "dropout = Dropout(0.1)(embedding)\n",
        "pooled_convolutions = []\n",
        "for kernel_size in [2, 3, 4, 5]:\n",
        "    convolution = Conv1D(filters=20, \n",
        "                         kernel_size=kernel_size,\n",
        "                         padding='valid',\n",
        "                         strides=1,\n",
        "                         activation='relu')(dropout)\n",
        "    pool = GlobalMaxPooling1D()(convolution)\n",
        "    pooled_convolutions.append(pool)\n",
        "concatenated = Concatenate()(pooled_convolutions)\n",
        "dropout = Dropout(0.5)(concatenated)\n",
        "dense = Dense(100, activation='relu')(dropout)\n",
        "dropout = Dropout(0.5)(dense)\n",
        "output = Dense(len(label_encoder.classes_), activation='softmax')(dense)\n",
        "\n",
        "conv_model = Model(inputs=input_text, outputs=output)\n",
        "\n",
        "conv_model.compile(optimizer='adam', \n",
        "                  loss='categorical_crossentropy', \n",
        "                  metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzuDoDJnBg_0",
        "colab_type": "code",
        "outputId": "2508e58c-b218-4bf9-a39b-bf10e2eaefa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "conv_model.fit(x=X_train_seq, y=y_train,\n",
        "               validation_data=(X_valid_seq, y_valid),\n",
        "               batch_size=32, epochs=5)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 18681 samples, validate on 9032 samples\n",
            "Epoch 1/5\n",
            "18681/18681 [==============================] - 10s 529us/step - loss: 1.9639 - acc: 0.5192 - val_loss: 1.0713 - val_acc: 0.7277\n",
            "Epoch 2/5\n",
            "18681/18681 [==============================] - 7s 352us/step - loss: 1.1288 - acc: 0.7140 - val_loss: 0.8714 - val_acc: 0.7730\n",
            "Epoch 3/5\n",
            "18681/18681 [==============================] - 7s 352us/step - loss: 0.9222 - acc: 0.7604 - val_loss: 0.8176 - val_acc: 0.7820\n",
            "Epoch 4/5\n",
            "18681/18681 [==============================] - 7s 353us/step - loss: 0.8017 - acc: 0.7878 - val_loss: 0.8143 - val_acc: 0.7855\n",
            "Epoch 5/5\n",
            "18681/18681 [==============================] - 7s 352us/step - loss: 0.7195 - acc: 0.8042 - val_loss: 0.8215 - val_acc: 0.7863\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7efd04071588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-nB7tcKAGkB",
        "colab_type": "text"
      },
      "source": [
        "# Next Lesson\n",
        "[Recurrent Neural Networks](https://colab.research.google.com/drive/1UI85j2DIkMXMgiHKduboCIRVasRZDwrJ)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMD64HQaAYbe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}